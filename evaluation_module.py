# evaluation_module.py ì „ì²´ íŒŒì¼ (ë©€í‹°í„´ ë²„ì „ v2 - ìƒˆë¡œìš´ ìš”êµ¬ì‚¬í•­ ë°˜ì˜)

# === SQL í‰ê°€ ê¸°ëŠ¥ + í† í° ì‚¬ìš©ëŸ‰ ì¶”ì  í†µí•© ëª¨ë“ˆ ===

import os  # íŒŒì¼ ì‹œìŠ¤í…œ ì ‘ê·¼
import sys  # ì‹œìŠ¤í…œ ê²½ë¡œ ê´€ë¦¬
import json  # JSON íŒŒì¼ ì²˜ë¦¬
from datetime import datetime  # ì‹œê°„ ì •ë³´
import cx_Oracle  # Oracle DB ì—°ê²°
import re  # ì •ê·œí‘œí˜„ì‹
from langsmith import Client
from langsmith.run_helpers import traceable
from langchain.callbacks.base import BaseCallbackHandler  # LangChain ì½œë°±
import hashlib
import time
from typing import Dict, Any, Optional

# === [1] í‰ê°€ ëª¨ë“ˆ import ===
sys.path.append('.')
from evaluation import Evaluator, eval_exec_match
from process_sql import get_sql, Schema

# === ğŸ”¥ SParC ê³µì‹ í•¨ìˆ˜ë“¤ ì¶”ê°€ ===

# í•¨ìˆ˜ ì •ê·œí™” í•¨ìˆ˜ë“¤
FORMATTING_FUNCTIONS = ('lower', 'upper', 'trim', 'ltrim', 'rtrim')

# === ë„ë©”ì¸ ë§¤í•‘ ì •ì˜ ===
DOMAIN_KEYWORDS = {
    'patients': ['í™˜ì', 'ë‚˜ì´', 'ì„±ë³„', 'ì…ì›', 'í‡´ì›', 'ì‚¬ë§', 'patient', 'age', 'gender', 'admit', 'discharge'],
    'diagproc': ['ì§„ë‹¨', 'ì§ˆë³‘', 'ICD', 'ì‹œìˆ ', 'ìˆ˜ìˆ ', 'diagnosis', 'procedure', 'surgery', 'disease'],
    'drugs': ['ì•½ë¬¼', 'ì²˜ë°©', 'íˆ¬ì•½', 'ìš©ëŸ‰', 'í•­ìƒì œ', 'drug', 'medication', 'prescription', 'dose'],
    'events': ['ê²€ì‚¬', 'ìˆ˜ì¹˜', 'ì¸¡ì •', 'ëª¨ë‹ˆí„°ë§', 'í˜ˆì••', 'ë§¥ë°•', 'lab', 'chart', 'vital', 'test'],
    'trial': ['ì‹œí—˜', 'ì—°êµ¬', 'ì„ìƒ', 'ì¹˜ë£Œíš¨ê³¼', 'trial', 'clinical', 'research', 'study']
}

SQL_COMPLEXITY_WEIGHTS = {
    'select': 1, 'from': 1, 'where': 2, 'join': 3, 'group': 3, 'order': 2,
    'having': 4, 'union': 4, 'intersect': 4, 'except': 4
}

# === í‰ê°€ ê´€ë ¨ ìƒìˆ˜ ì •ì˜ ===
STANDARD_CLAUSES = [
    'select', 'select(no AGG)', 'where', 'where(no OP)',
    'group(no Having)', 'group', 'order', 'and/or', 'IUEN', 'keywords'
]


# === ìŠ¤í‚¤ë§ˆ ê´€ë ¨ í•¨ìˆ˜ë“¤ ===
def extract_schema_dict_from_txt():
    """txt íŒŒì¼ë“¤ë¡œë¶€í„° ìŠ¤í‚¤ë§ˆ ë”•ì…”ë„ˆë¦¬ ìƒì„±"""
    schema_files = [
        "schema_patients.txt",
        "schema_diagproc.txt",
        "schema_drugs.txt",
        "schema_events.txt",
        "schema_trial.txt"
    ]

    combined_schema = {}

    for file_path in schema_files:
        if os.path.exists(file_path):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                # CREATE TABLE ë¬¸ì—ì„œ í…Œì´ë¸”ê³¼ ì»¬ëŸ¼ ì¶”ì¶œ
                table_matches = re.findall(r'CREATE TABLE (\w+)\s*\((.*?)\);', content, re.DOTALL | re.IGNORECASE)

                for table_name, columns_text in table_matches:
                    table_name = table_name.lower()

                    # ì»¬ëŸ¼ëª… ì¶”ì¶œ (ì»¬ëŸ¼ëª… ì»¬ëŸ¼íƒ€ì… í˜•íƒœ)
                    column_matches = re.findall(r'(\w+)\s+[A-Za-z0-9_\(\),\s]+', columns_text)
                    columns = [col.lower() for col in column_matches if
                               col.lower() not in ['constraint', 'primary', 'foreign', 'key', 'references']]

                    if columns:
                        combined_schema[table_name] = columns

            except Exception as e:
                print(f"Error reading {file_path}: {e}")
                continue

    print(f"âœ… ìŠ¤í‚¤ë§ˆ íŒŒì¼ ë¡œë”© ì™„ë£Œ: {len(combined_schema)}ê°œ í…Œì´ë¸”")
    return combined_schema


# === Oracle SQL ì •ê·œí™” ===
def normalize_oracle_sql_for_comparison(sql_str):
    """Oracle SQLì„ SParC í‰ê°€ìš©ìœ¼ë¡œ ì •ê·œí™”"""
    # 1. ê¸°ë³¸ ì „ì²˜ë¦¬
    sql = str(sql_str).strip()
    if not sql:
        return ""

    # 2. ëŒ€ì†Œë¬¸ì í†µì¼ (ëŒ€ë¬¸ìë¡œ)
    sql = sql.upper()

    # 3. ì„¸ë¯¸ì½œë¡  ì™„ì „ ì œê±°
    while sql.endswith(';'):
        sql = sql[:-1].strip()
    sql = re.sub(r';\s*;+', '', sql)  # ì—°ì† ì„¸ë¯¸ì½œë¡ 
    sql = re.sub(r';\s*$', '', sql)  # ë ì„¸ë¯¸ì½œë¡ 

    # 4. ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
    sql = re.sub(r'\s+', ' ', sql).strip()

    # 5. Oracle íŠ¹ìˆ˜ êµ¬ë¬¸ ì •ë¦¬
    # í…Œì´ë¸”ëª…ì— ìŠ¤í‚¤ë§ˆ ì œê±° (GPTify.PATIENTS -> PATIENTS)
    sql = re.sub(r'\bGPTify\.', '', sql, flags=re.IGNORECASE)

    # ROWNUM ì¡°ê±´ ì •ë¦¬ (ì„±ëŠ¥ ìµœì í™”ìš©ì´ë¯€ë¡œ í‰ê°€ì—ì„œ ì œì™¸)
    sql = re.sub(r'\s+WHERE\s+rownum\s*<=\s*\d+\s*$', '', sql, flags=re.IGNORECASE)
    sql = re.sub(r'\s+AND\s+rownum\s*<=\s*\d+', '', sql, flags=re.IGNORECASE)
    sql = re.sub(r'rownum\s*<=\s*\d+\s+AND\s+', '', sql, flags=re.IGNORECASE)

    return sql


# === í† í° ê´€ë¦¬ í´ë˜ìŠ¤ ===
class TokenCallback(BaseCallbackHandler):
    """LangChain í† í° ì‚¬ìš©ëŸ‰ ì¶”ì ì„ ìœ„í•œ ì½œë°± í´ë˜ìŠ¤"""

    def __init__(self):
        self.total_tokens = 0
        self.prompt_tokens = 0
        self.completion_tokens = 0

    def on_llm_start(self, serialized, prompts, **kwargs):
        pass

    def on_llm_end(self, response, **kwargs):
        if hasattr(response, 'llm_output') and response.llm_output:
            token_usage = response.llm_output.get('token_usage', {})
            self.total_tokens = token_usage.get('total_tokens', 0)
            self.prompt_tokens = token_usage.get('prompt_tokens', 0)
            self.completion_tokens = token_usage.get('completion_tokens', 0)


# ì „ì—­ í† í° ì½œë°± ì¸ìŠ¤í„´ìŠ¤
token_callback = TokenCallback()


def estimate_token_usage(text):
    """í…ìŠ¤íŠ¸ì˜ ëŒ€ëµì ì¸ í† í° ìˆ˜ ì¶”ì •"""
    if not text:
        return 0
    # ëŒ€ëµì ìœ¼ë¡œ í•œêµ­ì–´ëŠ” ê¸€ìë‹¹ 1.5í† í°, ì˜ì–´ëŠ” ë‹¨ì–´ë‹¹ 1.3í† í°ìœ¼ë¡œ ì¶”ì •
    korean_chars = len(re.findall(r'[ê°€-í£]', text))
    english_words = len(re.findall(r'\b[a-zA-Z]+\b', text))
    other_chars = len(text) - korean_chars - sum(len(word) for word in re.findall(r'\b[a-zA-Z]+\b', text))

    estimated_tokens = int(korean_chars * 1.5 + english_words * 1.3 + other_chars * 0.5)
    return max(estimated_tokens, 1)


def record_token_usage(user_question, generated_sql, response_text, estimated_tokens, actual_usage=None,
                       execution_success=False):
    """í† í° ì‚¬ìš©ëŸ‰ì„ ê¸°ë¡í•˜ëŠ” í•¨ìˆ˜"""
    try:
        token_record = {
            "timestamp": datetime.now().isoformat(),
            "user_question": user_question[:100],  # ì²˜ìŒ 100ìë§Œ ì €ì¥
            "generated_sql": generated_sql[:200] if generated_sql else "",
            "response_length": len(response_text) if response_text else 0,
            "estimated_tokens": estimated_tokens,
            "actual_tokens": actual_usage.get('total_tokens') if actual_usage else None,
            "execution_success": execution_success
        }

        # í† í° ë¡œê·¸ íŒŒì¼ì— ì €ì¥
        token_log_file = "token_usage_log.json"
        try:
            if os.path.exists(token_log_file):
                with open(token_log_file, 'r', encoding='utf-8') as f:
                    token_logs = json.load(f)
            else:
                token_logs = []

            token_logs.append(token_record)

            # ìµœê·¼ 1000ê°œë§Œ ìœ ì§€
            if len(token_logs) > 1000:
                token_logs = token_logs[-1000:]

            with open(token_log_file, 'w', encoding='utf-8') as f:
                json.dump(token_logs, f, indent=2, ensure_ascii=False)

        except Exception as file_error:
            print(f"í† í° ë¡œê·¸ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {file_error}")

        return token_record

    except Exception as e:
        print(f"í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡ ì‹¤íŒ¨: {e}")
        return None


def get_token_statistics():
    """í† í° ì‚¬ìš© í†µê³„ ì¡°íšŒ"""
    token_log_file = "token_usage_log.json"
    try:
        if not os.path.exists(token_log_file):
            return {"total_calls": 0, "total_estimated_tokens": 0}

        with open(token_log_file, 'r', encoding='utf-8') as f:
            token_logs = json.load(f)

        total_calls = len(token_logs)
        total_estimated = sum(log.get('estimated_tokens', 0) for log in token_logs)
        successful_executions = sum(1 for log in token_logs if log.get('execution_success', False))

        return {
            "total_calls": total_calls,
            "total_estimated_tokens": total_estimated,
            "successful_executions": successful_executions,
            "success_rate": successful_executions / total_calls if total_calls > 0 else 0
        }

    except Exception as e:
        print(f"í† í° í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {"error": str(e)}


# === SQL ì‹¤í–‰ ê²°ê³¼ ë¹„êµ ===
def compare_execution_results(generated_sql, target_sql, cache):
    """ìƒì„±ëœ SQLê³¼ ì •ë‹µ SQLì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë¹„êµ"""
    try:
        print(f"ğŸ” [EXEC_MATCH] ì‹¤í–‰ ê²°ê³¼ ë¹„êµ ì‹œì‘")

        # ìƒì„±ëœ SQL ì‹¤í–‰
        generated_result = run_sql_query_cached(generated_sql, cache)
        if not generated_result["success"]:
            print(f"âŒ ìƒì„± SQL ì‹¤í–‰ ì‹¤íŒ¨: {generated_result.get('error')}")
            return False

        # ì •ë‹µ SQL ì‹¤í–‰
        target_result = run_sql_query_cached(target_sql, cache)
        if not target_result["success"]:
            print(f"âŒ ì •ë‹µ SQL ì‹¤í–‰ ì‹¤íŒ¨: {target_result.get('error')}")
            return False

        # ğŸ” ê²°ê³¼ ë¹„êµ
        generated_rows = generated_result["result"]
        target_rows = target_result["result"]

        if len(generated_rows) != len(target_rows):
            print(f"ğŸ” [EXEC_MATCH] í–‰ ìˆ˜ ë¶ˆì¼ì¹˜: {len(generated_rows)} vs {len(target_rows)}")
            return False

        # ê° í–‰ì„ ì •ë ¬í•˜ì—¬ ë¹„êµ
        generated_sorted = sorted([tuple(row.values()) for row in generated_rows])
        target_sorted = sorted([tuple(row.values()) for row in target_rows])

        is_match = generated_sorted == target_sorted

        print(f"ğŸ” [EXEC_MATCH] ì‹¤í–‰ ê²°ê³¼ ë¹„êµ: {'âœ… ì¼ì¹˜' if is_match else 'âŒ ë¶ˆì¼ì¹˜'}")
        print(f"ğŸ” [EXEC_MATCH] ìƒì„±: {len(generated_rows):,}í–‰ vs ì •ë‹µ: {len(target_rows):,}í–‰")

        return is_match

    except Exception as e:
        print(f"âŒ [EXEC_MATCH] ì‹¤í–‰ ê²°ê³¼ ë¹„êµ ì˜¤ë¥˜: {e}")
        return False


# === SQL ê²°ê³¼ ìºì‹± í´ë˜ìŠ¤ ===
class SQLResultCache:
    """SQL ì‹¤í–‰ ê²°ê³¼ë¥¼ ìºì‹±í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, max_size: int = 1000, ttl_seconds: int = 3600):
        """
        Args:
            max_size: ìµœëŒ€ ìºì‹œ í¬ê¸° (ì¿¼ë¦¬ ê°œìˆ˜)
            ttl_seconds: ìºì‹œ ìœ íš¨ ì‹œê°„ (ì´ˆ, ê¸°ë³¸ 1ì‹œê°„)
        """
        self.cache: Dict[str, Dict[str, Any]] = {}
        self.max_size = max_size
        self.ttl_seconds = ttl_seconds
        self.hit_count = 0  # ìºì‹œ íˆíŠ¸ íšŸìˆ˜
        self.miss_count = 0  # ìºì‹œ ë¯¸ìŠ¤ íšŸìˆ˜

    def _generate_cache_key(self, sql: str) -> str:
        """SQL ë¬¸ìì—´ë¡œë¶€í„° ìºì‹œ í‚¤ ìƒì„±"""
        # SQLì„ ì •ê·œí™” (ê³µë°±, ëŒ€ì†Œë¬¸ì í†µì¼)
        normalized_sql = ' '.join(sql.strip().lower().split())
        # MD5 í•´ì‹œë¡œ ì§§ì€ í‚¤ ìƒì„±
        return hashlib.md5(normalized_sql.encode()).hexdigest()

    def _is_expired(self, cache_entry: Dict[str, Any]) -> bool:
        """ìºì‹œ í•­ëª©ì´ ë§Œë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        return time.time() - cache_entry['timestamp'] > self.ttl_seconds

    def _cleanup_expired(self):
        """ë§Œë£Œëœ ìºì‹œ í•­ëª©ë“¤ ì •ë¦¬"""
        current_time = time.time()
        expired_keys = [
            key for key, entry in self.cache.items()
            if current_time - entry['timestamp'] > self.ttl_seconds
        ]
        for key in expired_keys:
            del self.cache[key]

    def get(self, sql: str) -> Optional[Dict[str, Any]]:
        """ìºì‹œì—ì„œ SQL ì‹¤í–‰ ê²°ê³¼ ì¡°íšŒ"""
        cache_key = self._generate_cache_key(sql)

        if cache_key in self.cache:
            entry = self.cache[cache_key]
            if not self._is_expired(entry):
                self.hit_count += 1
                return entry['result']
            else:
                # ë§Œë£Œëœ í•­ëª© ì œê±°
                del self.cache[cache_key]

        self.miss_count += 1
        return None

    def put(self, sql: str, result: Dict[str, Any]):
        """SQL ì‹¤í–‰ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥"""
        # ìºì‹œ í¬ê¸° ê´€ë¦¬
        if len(self.cache) >= self.max_size:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª©ë¶€í„° ì œê±°
            oldest_key = min(self.cache.keys(),
                             key=lambda k: self.cache[k]['timestamp'])
            del self.cache[oldest_key]

        cache_key = self._generate_cache_key(sql)
        self.cache[cache_key] = {
            'result': result,
            'timestamp': time.time()
        }

    def get_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        total_requests = self.hit_count + self.miss_count
        hit_rate = self.hit_count / total_requests if total_requests > 0 else 0

        return {
            'cache_size': len(self.cache),
            'max_size': self.max_size,
            'hit_count': self.hit_count,
            'miss_count': self.miss_count,
            'hit_rate': hit_rate,
            'ttl_seconds': self.ttl_seconds
        }

    def clear(self):
        """ìºì‹œ ì „ì²´ ì‚­ì œ"""
        self.cache.clear()
        self.hit_count = 0
        self.miss_count = 0


def run_sql_query_cached(sql, cache):
    """ìºì‹œë¥¼ ì‚¬ìš©í•˜ì—¬ SQL ì¿¼ë¦¬ ì‹¤í–‰"""
    try:
        # ìºì‹œ í™•ì¸
        cached_result = cache.get(sql)
        if cached_result is not None:
            return cached_result

        # ìºì‹œì— ì—†ìœ¼ë©´ ì‹¤ì œ ì‹¤í–‰
        result = run_sql_query_direct(sql)

        # ì„±ê³µí•œ ê²°ê³¼ë§Œ ìºì‹œì— ì €ì¥
        if result.get("success", False):
            cache.put(sql, result)

        return result

    except Exception as e:
        return {
            "success": False,
            "error": f"ìºì‹œ SQL ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}",
            "result": []
        }


def run_sql_query_direct(sql):
    """SQL ì¿¼ë¦¬ë¥¼ ì§ì ‘ ì‹¤í–‰"""
    try:
        ORACLE_USER = os.getenv("ORACLE_USER", "GPTify")
        ORACLE_PW = os.getenv("ORACLE_PW", "oracle_4U")
        ORACLE_HOST = os.getenv("ORACLE_HOST", "138.2.63.245")
        ORACLE_PORT = int(os.getenv("ORACLE_PORT", "1521"))
        ORACLE_SERVICE = os.getenv("ORACLE_SERVICE", "srvinv.sub03250142080.kdtvcn.oraclevcn.com")

        dsn = cx_Oracle.makedsn(ORACLE_HOST, ORACLE_PORT, service_name=ORACLE_SERVICE)
        conn = cx_Oracle.connect(user=ORACLE_USER, password=ORACLE_PW, dsn=dsn)
        cursor = conn.cursor()

        cursor.execute(sql)
        columns = [desc[0] for desc in cursor.description]
        rows = cursor.fetchall()

        result = []
        for row in rows:
            row_dict = {}
            for i, value in enumerate(row):
                row_dict[columns[i]] = value
            result.append(row_dict)

        cursor.close()
        conn.close()

        return {
            "success": True,
            "result": result,
            "row_count": len(result)
        }

    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "result": []
        }


# ëª¨ë“ˆ ë ˆë²¨ì—ì„œ ì „ì—­ ìºì‹œ ê°ì²´ ìƒì„± (ì•± ì‹¤í–‰ ë™ì•ˆ ìœ ì§€)
sql_result_cache = SQLResultCache(
    max_size=100,  # ìµœëŒ€ 1000ê°œ ì¿¼ë¦¬ ê²°ê³¼ ìºì‹œ
    ttl_seconds=600  # 1ì‹œê°„ ë™ì•ˆ ìœ íš¨
)

print("ğŸ¯ SQL ê²°ê³¼ ìºì‹± ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")


def normalize_col_unit_semantic(col_unit1, col_unit2, schema):
    """
    ë‘ ì»¬ëŸ¼ ë‹¨ìœ„ë¥¼ ì˜ë¯¸ì ìœ¼ë¡œ ë¹„êµ (í•¨ìˆ˜ ë¬´ì‹œ)
    (agg_id, col_id, distinct) í˜•íƒœ ì²˜ë¦¬
    """
    if not col_unit1 or not col_unit2 or len(col_unit1) < 2 or len(col_unit2) < 2:
        return col_unit1 == col_unit2

    agg1, col1, distinct1 = col_unit1[0], col_unit1[1], col_unit1[2] if len(col_unit1) > 2 else False
    agg2, col2, distinct2 = col_unit2[0], col_unit2[1], col_unit2[2] if len(col_unit2) > 2 else False

    # ì§‘ê³„í•¨ìˆ˜ì™€ DISTINCTëŠ” ë™ì¼í•´ì•¼ í•¨
    if agg1 != agg2 or distinct1 != distinct2:
        return False

    # ì»¬ëŸ¼ IDê°€ ì™„ì „íˆ ê°™ìœ¼ë©´ True
    if col1 == col2:
        return True

    # ìŠ¤í‚¤ë§ˆì—ì„œ ì‹¤ì œ ì»¬ëŸ¼ëª… ì¶”ì¶œí•˜ì—¬ ë¹„êµ
    col1_name = extract_column_name_from_id(col1)
    col2_name = extract_column_name_from_id(col2)

    return col1_name == col2_name


def extract_column_name_from_id(col_id):
    """
    ì»¬ëŸ¼ IDì—ì„œ ì‹¤ì œ ì»¬ëŸ¼ëª… ì¶”ì¶œ
    __prescriptions.drug__ â†’ drug
    __all__ â†’ *
    """
    if not isinstance(col_id, str):
        return str(col_id)

    if isinstance(col_id, str):
        col_id = col_id.strip('_')
    else:
        col_id = str(col_id)

    # __all__ ì²˜ë¦¬
    if col_id == 'all':
        return '*'

    # í…Œì´ë¸”.ì»¬ëŸ¼ í˜•íƒœì—ì„œ ì»¬ëŸ¼ëª…ë§Œ ì¶”ì¶œ
    if '.' in col_id:
        parts = col_id.split('.')
        return parts[-1].strip('_')

    return col_id


def normalize_val_unit_semantic(val_unit1, val_unit2, schema):
    """
    ë‘ ê°’ ë‹¨ìœ„ë¥¼ ì˜ë¯¸ì ìœ¼ë¡œ ë¹„êµ
    (unit_op, col_unit1, col_unit2) í˜•íƒœ ì²˜ë¦¬
    """
    if not val_unit1 or not val_unit2 or len(val_unit1) < 2 or len(val_unit2) < 2:
        return val_unit1 == val_unit2

    op1, col1, col2_1 = val_unit1[0], val_unit1[1], val_unit1[2] if len(val_unit1) > 2 else None
    op2, col1_2, col2_2 = val_unit2[0], val_unit2[1], val_unit2[2] if len(val_unit2) > 2 else None

    # ì—°ì‚°ìëŠ” ë™ì¼í•´ì•¼ í•¨
    if op1 != op2:
        return False

    # ì²« ë²ˆì§¸ ì»¬ëŸ¼ ë‹¨ìœ„ ë¹„êµ
    if not normalize_col_unit_semantic(col1, col1_2, schema):
        return False

    # ë‘ ë²ˆì§¸ ì»¬ëŸ¼ ë‹¨ìœ„ ë¹„êµ (ìˆëŠ” ê²½ìš°)
    if col2_1 is None and col2_2 is None:
        return True
    elif col2_1 is not None and col2_2 is not None:
        return normalize_col_unit_semantic(col2_1, col2_2, schema)
    else:
        return False  # í•˜ë‚˜ë§Œ Noneì¸ ê²½ìš°


# === ì¤‘ì²© SQL ê´€ë ¨ í•¨ìˆ˜ë“¤ ===
def get_nestedSQL(sql):
    """ì¤‘ì²© SQL(ì„œë¸Œì¿¼ë¦¬) ì¶”ì¶œ í•¨ìˆ˜"""
    nested = []
    for cond_unit in sql['from']['conds'][::2] + sql['where'][::2] + sql['having'][::2]:
        if len(cond_unit) >= 5:
            if type(cond_unit[3]) is dict:
                nested.append(cond_unit[3])
            if type(cond_unit[4]) is dict:
                nested.append(cond_unit[4])

    if sql['intersect'] is not None:
        nested.append(sql['intersect'])
    if sql['except'] is not None:
        nested.append(sql['except'])
    if sql['union'] is not None:
        nested.append(sql['union'])

    return nested


def has_agg(unit):
    """ë‹¨ìœ„ì— ì§‘ê³„ í•¨ìˆ˜ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜"""
    AGG_OPS = ('none', 'max', 'min', 'count', 'sum', 'avg')
    return unit[0] != AGG_OPS.index('none')


def count_agg(units):
    """ì§‘ê³„ í•¨ìˆ˜ ê°œìˆ˜ ì„¸ê¸° í•¨ìˆ˜"""
    return len([unit for unit in units if has_agg(unit)])


def count_component1(sql):
    """ê¸°ë³¸ ì»´í¬ë„ŒíŠ¸ ê°œìˆ˜ ì„¸ê¸° í•¨ìˆ˜ (SParC ê³µì‹)"""
    count = 0
    WHERE_OPS = ('not', 'between', '=', '>', '<', '>=', '<=', '!=', 'in', 'like', 'is', 'exists')

    if len(sql['where']) > 0:
        count += 1
    if len(sql['groupBy']) > 0:
        count += 1
    if len(sql['orderBy']) > 0:
        count += 1
    if sql['limit'] is not None:
        count += 1
    if len(sql['from']['table_units']) > 0:
        count += len(sql['from']['table_units']) - 1

    # OR ê°œìˆ˜ ì¶”ê°€
    ao = sql['from']['conds'][1::2] + sql['where'][1::2] + sql['having'][1::2]
    count += len([token for token in ao if token == 'or'])

    # LIKE ê°œìˆ˜ ì¶”ê°€
    cond_units = sql['from']['conds'][::2] + sql['where'][::2] + sql['having'][::2]
    count += len([cond_unit for cond_unit in cond_units
                  if len(cond_unit) > 1 and cond_unit[1] == WHERE_OPS.index('like')])

    return count


def count_component2(sql):
    """ê³ ê¸‰ ì»´í¬ë„ŒíŠ¸ ê°œìˆ˜ ì„¸ê¸° í•¨ìˆ˜ (ì¤‘ì²© SQL ê°œìˆ˜)"""
    nested = get_nestedSQL(sql)
    return len(nested)


def count_others(sql):
    """ê¸°íƒ€ ë³µì¡ë„ ìš”ì†Œ ê°œìˆ˜ ì„¸ê¸° í•¨ìˆ˜"""
    count = 0

    # ì§‘ê³„ í•¨ìˆ˜ ê°œìˆ˜ ê³„ì‚°
    agg_count = count_agg(sql['select'][1])
    agg_count += count_agg(sql['where'][::2])
    agg_count += count_agg(sql['groupBy'])

    if len(sql['orderBy']) > 0:
        order_val_units = sql['orderBy'][1] if len(sql['orderBy']) > 1 else []
        for val_unit in order_val_units:
            if val_unit and len(val_unit) > 1:
                if val_unit[1] and has_agg(val_unit[1]):
                    agg_count += 1
                if len(val_unit) > 2 and val_unit[2] and has_agg(val_unit[2]):
                    agg_count += 1

    agg_count += count_agg(sql['having'])

    if agg_count > 1:
        count += 1

    # SELECT ì»¬ëŸ¼ ê°œìˆ˜
    if len(sql['select'][1]) > 1:
        count += 1

    # WHERE ì¡°ê±´ ê°œìˆ˜
    if len(sql['where']) > 1:
        count += 1

    # GROUP BY ì ˆ ê°œìˆ˜
    if len(sql['groupBy']) > 1:
        count += 1

    return count


def create_empty_sql_structure():
    """SParC ë°©ì‹ìœ¼ë¡œ íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  ë¹ˆ SQL êµ¬ì¡° ìƒì„±"""
    return {
        "except": None,
        "from": {
            "conds": [],
            "table_units": []
        },
        "groupBy": [],
        "having": [],
        "intersect": None,
        "limit": None,
        "orderBy": [],
        "select": [
            False,
            []
        ],
        "union": None,
        "where": []
    }


# === ë©€í‹°í„´ ì„¸ì…˜ í´ë˜ìŠ¤ ===
class MultiTurnSession:
    """ë©€í‹°í„´ ëŒ€í™” ì„¸ì…˜ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, session_id, max_turns=5):
        self.session_id = session_id
        self.max_turns = max_turns
        self.turns = []
        self.status = "ì§„í–‰ì¤‘"
        self.created_at = datetime.now().isoformat()
        self.completed_at = None
        self.total_tokens = 0
        self.session_token_history = []
        self.session_start_time = time.time()
        self.session_end_time = None
        self.session_duration = None

    def add_turn(self, turn_data):
        """ìƒˆë¡œìš´ í„´ ì¶”ê°€"""
        turn_data['turn_number'] = len(self.turns) + 1
        turn_data['timestamp'] = datetime.now().isoformat()
        self.turns.append(turn_data)

        # ğŸ”¥ í„´ ì œí•œì— ë„ë‹¬í–ˆì„ ë•Œë§Œ ì™„ë£Œ
        if len(self.turns) >= self.max_turns:
            self.status = "ì™„ë£Œ"
            self.completed_at = datetime.now().isoformat()

    def get_efficiency(self):
        """íš¨ìœ¨ì„± ê³„ì‚° (í˜„ì¬ëŠ” ì˜ë¯¸ ì—†ìŒ, í˜¸í™˜ì„± ìœ ì§€)"""
        return 0.0  # ìë™ ì™„ë£Œ ì—†ìœ¼ë¯€ë¡œ íš¨ìœ¨ì„± ê°œë… ì—†ìŒ

    def to_dict(self):
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (JSON ì €ì¥ìš©)"""
        session_duration = getattr(self, 'session_duration', None)
        if session_duration is None:
            # session_start_timeê³¼ session_end_timeìœ¼ë¡œ ê³„ì‚° ì‹œë„
            start_time = getattr(self, 'session_start_time', None)
            end_time = getattr(self, 'session_end_time', None)
            if start_time and end_time:
                session_duration = end_time - start_time
            else:
                session_duration = 0.0
        print(f"ğŸ” [TO_DICT] {self.session_id}: session_duration = {session_duration}")

        return {
            'session_id': self.session_id,
            'max_turns': self.max_turns,
            'turns': self.turns,
            'status': self.status,
            'created_at': self.created_at,
            'completed_at': self.completed_at,
            'efficiency': self.get_efficiency(),
            # ğŸ”¥ ì¶”ê°€ í•„ìš”
            'total_tokens': getattr(self, 'total_tokens', 0),
            'session_token_history': getattr(self, 'session_token_history', []),
            # ğŸ”¥ ì‹œê°„ í•„ë“œ ì¶”ê°€
            'session_start_time': getattr(self, 'session_start_time', None),
            'session_end_time': getattr(self, 'session_end_time', None),
            'session_duration': getattr(self, 'session_duration', 0.0) if hasattr(self, 'session_duration') and getattr(
                self, 'session_duration') is not None else 0.0
        }


class ClauseProgressAnalyzer:
    """ğŸ”¥ SParC ê³µì‹ í‰ê°€ ë¡œì§ ê¸°ë°˜ Clauseë³„ ì§„í–‰ ìƒí™© ë¶„ì„ê¸°"""

    def __init__(self, evaluator, schema):
        self.evaluator = evaluator
        self.schema = schema

    def analyze_clause_progress(self, generated_sql, target_sql):
        """
        ìƒì„±ëœ SQLê³¼ ëª©í‘œ SQLì„ ë¹„êµí•˜ì—¬ ê° ì ˆì˜ ì™„ì„±ë„ ê³„ì‚°
        ğŸ”¥ SParC ê³µì‹ í‰ê°€ í•¨ìˆ˜ë“¤ì„ ì§ì ‘ ì‚¬ìš© (0 ë˜ëŠ” 1ë§Œ ë°˜í™˜)
        """
        try:
            # ğŸ”¥ SQL ì •ê·œí™” í›„ íŒŒì‹± (exact matchì™€ ë™ì¼í•˜ê²Œ)
            normalized_generated = normalize_oracle_sql_for_comparison(generated_sql)
            normalized_target = normalize_oracle_sql_for_comparison(target_sql)

            generated_parsed = get_sql(self.schema, normalized_generated)
            target_parsed = get_sql(self.schema, normalized_target)

            # ğŸ”¥ í•µì‹¬ ë³€ê²½: Evaluatorì˜ ê³µì‹ í•¨ìˆ˜ë“¤ ì§ì ‘ ì‚¬ìš©
            from evaluation import eval_select, eval_where, eval_group, eval_having, eval_order, eval_and_or, \
                eval_nested, eval_IUEN, eval_keywords

            # SParC ê³µì‹ ì ˆë³„ í‰ê°€ ìˆ˜í–‰
            select_scores = eval_select(generated_parsed, target_parsed, self.schema)
            where_scores = eval_where(generated_parsed, target_parsed, self.schema)
            group_scores = eval_group(generated_parsed, target_parsed)
            having_scores = eval_having(generated_parsed, target_parsed)
            order_scores = eval_order(generated_parsed, target_parsed, self.schema)
            and_or_scores = eval_and_or(generated_parsed, target_parsed)
            iuen_scores = eval_IUEN(generated_parsed, target_parsed)
            keyword_scores = eval_keywords(generated_parsed, target_parsed)

            # ğŸ”¥ SParC ë°©ì‹: 0 ë˜ëŠ” 1ë§Œ ë°˜í™˜ (ì™„ì „ ì´ì§„ í‰ê°€)
            def calculate_binary_score(label_total, pred_total, cnt, cnt_wo_agg=None):
                """SParC ê³µì‹ ì ìˆ˜ ê³„ì‚° ë¡œì§"""
                if label_total == 0 and pred_total == 0:
                    return None  # ì‚¬ìš©í•˜ì§€ ì•Šì€ ì ˆì€ í‰ê°€ ì œì™¸
                elif pred_total != label_total:
                    return 0  # ê°œìˆ˜ ë¶ˆì¼ì¹˜ â†’ 0ì 
                elif cnt == pred_total:
                    return 1  # ì™„ì „ ì¼ì¹˜ â†’ 1ì 
                else:
                    return 0  # ë¶€ë¶„ ì¼ì¹˜ â†’ 0ì 

            # ê° ì ˆë³„ ì ìˆ˜ ê³„ì‚°
            clause_progress = {}

            # SELECT ì ˆ
            if len(select_scores) >= 3:
                clause_progress['select'] = calculate_binary_score(select_scores[0], select_scores[1], select_scores[2])
            if len(select_scores) >= 4:
                clause_progress['select(no AGG)'] = calculate_binary_score(select_scores[0], select_scores[1],
                                                                           select_scores[3])

            # WHERE ì ˆ
            if len(where_scores) >= 3:
                clause_progress['where'] = calculate_binary_score(where_scores[0], where_scores[1], where_scores[2])
            if len(where_scores) >= 4:
                clause_progress['where(no OP)'] = calculate_binary_score(where_scores[0], where_scores[1],
                                                                         where_scores[3])

            # GROUP BY ì ˆ
            if len(group_scores) >= 3:
                clause_progress['group(no Having)'] = calculate_binary_score(group_scores[0], group_scores[1],
                                                                             group_scores[2])
            if len(having_scores) >= 3:
                clause_progress['group'] = calculate_binary_score(having_scores[0], having_scores[1], having_scores[2])

            # ORDER BY ì ˆ
            if len(order_scores) >= 3:
                clause_progress['order'] = calculate_binary_score(order_scores[0], order_scores[1], order_scores[2])

            # AND/OR ì ˆ
            if len(and_or_scores) >= 3:
                clause_progress['and/or'] = calculate_binary_score(and_or_scores[0], and_or_scores[1], and_or_scores[2])

            # IUEN (INTERSECT/UNION/EXCEPT/NESTED)
            if len(iuen_scores) >= 3:
                clause_progress['IUEN'] = calculate_binary_score(iuen_scores[0], iuen_scores[1], iuen_scores[2])

            # Keywords
            if len(keyword_scores) >= 3:
                clause_progress['keywords'] = calculate_binary_score(keyword_scores[0], keyword_scores[1],
                                                                     keyword_scores[2])

            return clause_progress

        except Exception as e:
            print(f"âŒ [CLAUSE_PROGRESS] ì ˆë³„ ì§„í–‰ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ëª¨ë“  ì ˆì„ 0ì ìœ¼ë¡œ ì²˜ë¦¬
            return {clause: 0 for clause in STANDARD_CLAUSES}


# === ë©€í‹°í„´ í‰ê°€ ê´€ë¦¬ì í´ë˜ìŠ¤ ===
class MultiTurnEvaluationManager:
    """ë©€í‹°í„´ í‰ê°€ë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, sql_evaluator):
        self.sql_evaluator = sql_evaluator
        self.current_session = None
        self.session_file = "multiturn_sessions.json"
        self.clause_analyzer = ClauseProgressAnalyzer(sql_evaluator.evaluator, sql_evaluator.schema)

    def start_new_session(self, max_turns=5):
        """ìƒˆë¡œìš´ ë©€í‹°í„´ ì„¸ì…˜ ì‹œì‘"""
        session_id = f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.current_session = MultiTurnSession(session_id, max_turns)
        print(f"ğŸ¯ [MULTITURN] ìƒˆ ì„¸ì…˜ ì‹œì‘: {session_id} (ìµœëŒ€ {max_turns}í„´)")
        return session_id

    def add_turn_to_session(self, user_question, generated_sql, target_sql=None, token_usage=None):
        """í˜„ì¬ ì„¸ì…˜ì— ìƒˆë¡œìš´ í„´ ì¶”ê°€"""
        if not self.current_session:
            print("âŒ [MULTITURN] í™œì„± ì„¸ì…˜ì´ ì—†ìŒ. ìƒˆ ì„¸ì…˜ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            self.start_new_session()

        turn_data = {
            'user_question': user_question,
            'generated_sql': generated_sql,
            'target_sql': target_sql or "",
            'exact_match': False,
            'execution_match': False,
            'clause_progress': {}
        }

        # ê¸°ë³¸ í‰ê°€ ìˆ˜í–‰
        if generated_sql and target_sql:
            try:
                # Exact Match í‰ê°€
                normalized_generated = normalize_oracle_sql_for_comparison(generated_sql)
                normalized_target = normalize_oracle_sql_for_comparison(target_sql)

                if normalized_generated == normalized_target:
                    turn_data['exact_match'] = True
                    print("âœ… [MULTITURN] Exact Match ì„±ê³µ")
                else:
                    print("âŒ [MULTITURN] Exact Match ì‹¤íŒ¨")

                # Execution Match í‰ê°€
                exec_match = compare_execution_results(generated_sql, target_sql, sql_result_cache)
                turn_data['execution_match'] = exec_match
                print(f"ğŸ” [MULTITURN] Execution Match: {'âœ… ì„±ê³µ' if exec_match else 'âŒ ì‹¤íŒ¨'}")

                # ì ˆë³„ ì§„í–‰ë„ ë¶„ì„
                clause_progress = self.clause_analyzer.analyze_clause_progress(generated_sql, target_sql)
                turn_data['clause_progress'] = clause_progress
                print(f"ğŸ” [MULTITURN] ì ˆë³„ ì§„í–‰ë„ ë¶„ì„ ì™„ë£Œ")

            except Exception as e:
                print(f"âŒ [MULTITURN] í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")

        # RAG í‰ê°€ ê²°ê³¼ ì¶”ê°€
        if hasattr(self.sql_evaluator, 'last_rag_evaluation') and self.sql_evaluator.last_rag_evaluation:
            turn_data.update(self.sql_evaluator.last_rag_evaluation)

        # ì„¸ì…˜ í† í° ëˆ„ì 
        if token_usage and 'total_tokens' in token_usage:
            self.current_session.total_tokens += token_usage['total_tokens']
            self.current_session.session_token_history.append({
                'turn': len(self.current_session.turns) + 1,
                'tokens': token_usage['total_tokens']
            })

        # SQL íŒŒì‹± ë° í‰ê°€ ìˆ˜í–‰
        try:
            # ğŸ”¥ SParC ë°©ì‹ íŒŒì‹± ì‹œë„ (ì—„ê²©í•œ ì²˜ë¦¬)
            if generated_sql and generated_sql.strip():
                try:
                    normalized_generated = normalize_oracle_sql_for_comparison(generated_sql)
                    parsed_generated = get_sql(self.sql_evaluator.schema, normalized_generated)
                    turn_data['parsing_success'] = True
                    print("âœ… [PARSING] ìƒì„± SQL íŒŒì‹± ì„±ê³µ")

                except Exception as parse_error:
                    # ğŸ”¥ SParC ë°©ì‹: íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ì‹¤íŒ¨ ì²˜ë¦¬
                    turn_data['parsing_success'] = False
                    turn_data['parsing_error_detail'] = str(parse_error)
                    turn_data['exact_match'] = False
                    turn_data['execution_match'] = False
                    print(f"âŒ [PARSING] ìƒì„± SQL íŒŒì‹± ì‹¤íŒ¨: {parse_error}")
                    print("ğŸ” [SPARC_MODE] íŒŒì‹± ì‹¤íŒ¨ë¡œ ì¸í•´ ëª¨ë“  í‰ê°€ 0ì  ì²˜ë¦¬")

                    # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¹ˆ SQL êµ¬ì¡° ìƒì„± (SParC ë°©ì‹)
                    parsed_generated = {
                        "except": None,
                        "from": {"conds": [], "table_units": []},
                        "groupBy": [],
                        "having": [],
                        "intersect": None,
                        "limit": None,
                        "orderBy": [],
                        "select": [False, []],
                        "union": None,
                        "where": []
                    }

            # ğŸ”¥ ì •ë‹µ SQLë„ ë™ì¼í•˜ê²Œ ì—„ê²©í•˜ê²Œ ì²˜ë¦¬
            if target_sql and target_sql.strip():
                try:
                    normalized_target = normalize_oracle_sql_for_comparison(target_sql)
                    parsed_target = get_sql(self.sql_evaluator.schema, normalized_target)
                    print("âœ… [PARSING] ì •ë‹µ SQL íŒŒì‹± ì„±ê³µ")

                except Exception as target_parse_error:
                    print(f"âŒ [PARSING] ì •ë‹µ SQL íŒŒì‹± ì‹¤íŒ¨: {target_parse_error}")
                    # ì •ë‹µ SQL íŒŒì‹± ì‹¤íŒ¨ ì‹œë„ ë¹ˆ êµ¬ì¡°ë¡œ ëŒ€ì²´
                    parsed_target = create_empty_sql_structure()

        except Exception as overall_parsing_error:
            print(f"âŒ [PARSING] ì „ì²´ íŒŒì‹± ê³¼ì • ì‹¤íŒ¨: {overall_parsing_error}")
            turn_data['parsing_success'] = False
            turn_data['parsing_error_detail'] = str(overall_parsing_error)

        # í„´ ì¶”ê°€
        self.current_session.add_turn(turn_data)

        # ì„¸ì…˜ì´ ì™„ë£Œë˜ë©´ ì €ì¥
        if self.current_session.status == "ì™„ë£Œ":
            self.current_session.session_end_time = time.time()
            self.current_session.session_duration = self.current_session.session_end_time - self.current_session.session_start_time
            self.save_session()
            print(f"ğŸ‰ [MULTITURN] ì„¸ì…˜ ì™„ë£Œ ë° ì €ì¥: {self.current_session.session_id}")

        return len(self.current_session.turns)

    def save_session(self):
        """í˜„ì¬ ì„¸ì…˜ì„ íŒŒì¼ì— ì €ì¥"""
        if not self.current_session:
            return

        try:
            # ê¸°ì¡´ ì„¸ì…˜ë“¤ ë¡œë“œ
            if os.path.exists(self.session_file):
                with open(self.session_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
            else:
                data = {"multiturn_sessions": []}

            # í˜„ì¬ ì„¸ì…˜ ì¶”ê°€
            sessions = data.get("multiturn_sessions", [])

            # ë™ì¼í•œ session_idê°€ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ì¶”ê°€
            session_dict = self.current_session.to_dict()
            existing_index = None
            for i, session in enumerate(sessions):
                if session.get('session_id') == self.current_session.session_id:
                    existing_index = i
                    break

            if existing_index is not None:
                sessions[existing_index] = session_dict
                print(f"ğŸ“ [SAVE] ê¸°ì¡´ ì„¸ì…˜ ì—…ë°ì´íŠ¸: {self.current_session.session_id}")
            else:
                sessions.append(session_dict)
                print(f"ğŸ“ [SAVE] ìƒˆ ì„¸ì…˜ ì¶”ê°€: {self.current_session.session_id}")

            # íŒŒì¼ì— ì €ì¥
            data["multiturn_sessions"] = sessions
            with open(self.session_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)

            print(f"âœ… [SAVE] ì„¸ì…˜ ì €ì¥ ì™„ë£Œ: {len(sessions)}ê°œ ì„¸ì…˜")

            # ğŸ”¥ ì €ì¥ ê²€ì¦
            try:
                with open(self.session_file, 'r', encoding='utf-8') as f:
                    verify_data = json.load(f)
                verify_sessions = verify_data.get('multiturn_sessions', [])
                print(f"âœ… [SAVE_VERIFY] ì €ì¥ ê²€ì¦ ì„±ê³µ: {len(verify_sessions)}ê°œ ì„¸ì…˜ í™•ì¸")
            except Exception as verify_error:
                print(f"âŒ [SAVE_VERIFY] ì €ì¥ ê²€ì¦ ì‹¤íŒ¨: {verify_error}")

        except Exception as e:
            print(f"âŒ ì„¸ì…˜ ì €ì¥ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()


# === [5] SQL í‰ê°€ ë©”ì¸ í´ë˜ìŠ¤ ===
class SQLEvaluationModule:
    # SQL í‰ê°€ + í† í° ì¶”ì  í†µí•© ë©”ì¸ í´ë˜ìŠ¤

    def __init__(self):
        # SQL í‰ê°€ ê´€ë ¨ ë³€ìˆ˜
        self.last_rag_evaluation = {}

        self.schema = None
        self.evaluator = None
        self.kmaps = None

        # íŒŒì¼ ê²½ë¡œ ì„¤ì •
        self.gold_file = "gold_queries.json"
        self.generated_file = "generated_queries.json"
        self.evaluation_file = "evaluation_results.json"
        self.token_log_file = "token_usage_test.json"

        # Schema íŒŒì¼ ëª©ë¡ (ì„¤ì • ê°€ëŠ¥)
        self.schema_files = [
            "schema_patients.txt",
            "schema_diagproc.txt",
            "schema_drugs.txt",
            "schema_events.txt",
            "schema_trial.txt"
        ]

        # ê²°ê³¼ ì €ì¥ ë³€ìˆ˜ (ë©€í‹°í„´ìš©ìœ¼ë¡œ ìˆ˜ì •)
        self.last_aggregate_result = ""

        # í† í° ì¶”ì  ê´€ë ¨ ë³€ìˆ˜
        self.token_log_file = "token_usage_log.json"
        self.session_tokens = {
            "total_tokens": 0,
            "api_calls": 0,
            "session_start": datetime.now().isoformat()
        }
        self.current_token_info = {}

        self.initialize()

    def initialize(self):
        """í‰ê°€ ëª¨ë“ˆ ì´ˆê¸°í™”"""
        try:
            # === ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™” ===
            print("ğŸ” ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™” ì‹œì‘")

            schema_dict = extract_schema_dict_from_txt()

            if schema_dict:
                self.schema = Schema(schema_dict)
                print(f"âœ… ìŠ¤í‚¤ë§ˆ ìƒì„± ì™„ë£Œ: {len(schema_dict)}ê°œ í…Œì´ë¸”")
            else:
                print("âŒ ìŠ¤í‚¤ë§ˆ ë”•ì…”ë„ˆë¦¬ê°€ ë¹„ì–´ìˆìŒ")
                self.schema = None

            # === Evaluator ì´ˆê¸°í™” ===
            try:
                self.evaluator = Evaluator(self.schema)
                print("âœ… Evaluator ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as eval_error:
                print(f"âŒ Evaluator ì´ˆê¸°í™” ì‹¤íŒ¨: {eval_error}")
                self.evaluator = None

            # === Foreign key map ì´ˆê¸°í™” ===
            self.kmaps = self.build_foreign_key_map_for_tables()
            self.db = "mimic"

            # === ë©€í‹°í„´ í‰ê°€ ê´€ë¦¬ì ì´ˆê¸°í™” ===
            try:
                self.multiturn_manager = MultiTurnEvaluationManager(self)
                print("âœ… ë©€í‹°í„´ í‰ê°€ ê´€ë¦¬ì ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ ë©€í‹°í„´ í‰ê°€ ê´€ë¦¬ì ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.multiturn_manager = None

        except Exception as e:
            print(f"âŒ í‰ê°€ ëª¨ë“ˆ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    def build_foreign_key_map_for_tables(self):
        """í…Œì´ë¸” ê°„ì˜ ì™¸ë˜í‚¤ ê´€ê³„ë¥¼ ë§¤í•‘í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ìƒì„±"""
        # MIMIC-IV ë°ì´í„°ë² ì´ìŠ¤ì˜ ì£¼ìš” ì™¸ë˜í‚¤ ê´€ê³„
        foreign_key_map = {
            "patients": {
                "subject_id": ["admissions.subject_id", "chartevents.subject_id", "prescriptions.subject_id"]
            },
            "admissions": {
                "subject_id": ["patients.subject_id"],
                "hadm_id": ["chartevents.hadm_id", "prescriptions.hadm_id", "diagnoses_icd.hadm_id"]
            },
            "chartevents": {
                "subject_id": ["patients.subject_id"],
                "hadm_id": ["admissions.hadm_id"],
                "itemid": ["d_items.itemid"]
            },
            "prescriptions": {
                "subject_id": ["patients.subject_id"],
                "hadm_id": ["admissions.hadm_id"]
            },
            "diagnoses_icd": {
                "subject_id": ["patients.subject_id"],
                "hadm_id": ["admissions.hadm_id"],
                "icd_code": ["d_icd_diagnoses.icd_code"]
            },
            "d_icd_diagnoses": {
                "icd_code": ["diagnoses_icd.icd_code"]
            },
            "d_items": {
                "itemid": ["chartevents.itemid"]
            }
        }
        return foreign_key_map

    def evaluate_and_save(self, user_question, generated_sql, gold_sql=None, exec_success=False, result_count=0):
        """í†µí•© í‰ê°€ í•¨ìˆ˜ - ê¸°ë³¸ í‰ê°€ + ë©€í‹°í„´ í‰ê°€ ëª¨ë‘ ìˆ˜í–‰"""
        try:
            print(f"ğŸ” [EVAL] í†µí•© í‰ê°€ ì‹œì‘")
            print(f"  â”” ì§ˆë¬¸: {user_question[:50]}...")
            print(f"  â”” ìƒì„± SQL: {generated_sql[:50] if generated_sql else 'None'}...")
            print(f"  â”” ì •ë‹µ SQL: {gold_sql[:50] if gold_sql else 'None'}...")

            # === ğŸ”¥ ë¬¸ì œ 1: ë©€í‹°í„´ í‰ê°€ ë¨¼ì € ìˆ˜í–‰ ===
            try:
                if self.multiturn_manager and generated_sql:
                    turn_number = self.multiturn_manager.add_turn_to_session(
                        user_question=user_question,
                        generated_sql=generated_sql,
                        target_sql=gold_sql,
                        token_usage=getattr(self, 'current_token_info', {})
                    )
                    print(f"âœ… [EVAL] ë©€í‹°í„´ í‰ê°€ ì™„ë£Œ (í„´ {turn_number})")
                else:
                    print(f"âš ï¸ [EVAL] ë©€í‹°í„´ ê´€ë¦¬ì ì—†ìŒ ë˜ëŠ” SQL ì—†ìŒ")
            except Exception as multiturn_error:
                print(f"âŒ [EVAL] ë©€í‹°í„´ í‰ê°€ ì‹¤íŒ¨: {multiturn_error}")

            # === ğŸ”¥ ë¬¸ì œ 2: ê¸°ë³¸ í‰ê°€ ìˆ˜í–‰ ===
            try:
                # í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ìƒì„±
                eval_result = {
                    "user_question": user_question,
                    "generated_sql": generated_sql,
                    "gold_sql": gold_sql or "",
                    "timestamp": datetime.now().isoformat(),
                    "execution_success": exec_success,
                    "result_count": result_count
                }

                # ğŸ”¥ ë¬¸ì œ 3: SQLì´ ìˆì„ ë•Œë§Œ íŒŒì‹± ë° í‰ê°€ ìˆ˜í–‰
                if generated_sql and generated_sql.strip():
                    try:
                        # SQL íŒŒì‹± ì‹œë„
                        normalized_sql = normalize_oracle_sql_for_comparison(generated_sql)
                        parsed_sql = get_sql(self.schema, normalized_sql)
                        eval_result['syntax_correct'] = True
                        eval_result['sql_normalized'] = normalized_sql
                        print(f"âœ… [EVAL] SQL íŒŒì‹± ì„±ê³µ")

                        # ğŸ”¥ ë¬¸ì œ 4: Gold SQLê³¼ ë¹„êµ í‰ê°€
                        if gold_sql and gold_sql.strip():
                            try:
                                # Exact Match í‰ê°€
                                normalized_gold = normalize_oracle_sql_for_comparison(gold_sql)
                                if normalized_sql == normalized_gold:
                                    eval_result['exact_match'] = True
                                    print(f"âœ… [EVAL] Exact Match ì„±ê³µ")
                                else:
                                    eval_result['exact_match'] = False
                                    print(f"âŒ [EVAL] Exact Match ì‹¤íŒ¨")

                                # ğŸ”¥ ë¬¸ì œ 5: Execution Match í‰ê°€
                                try:
                                    exec_match = compare_execution_results(generated_sql, gold_sql, sql_result_cache)
                                    eval_result['execution_match'] = exec_match
                                    print(f"ğŸ” [EVAL] Execution Match: {'âœ… ì„±ê³µ' if exec_match else 'âŒ ì‹¤íŒ¨'}")
                                except Exception as exec_error:
                                    print(f"âŒ [EVAL] Execution Match í‰ê°€ ì‹¤íŒ¨: {exec_error}")
                                    eval_result['execution_match'] = False

                            except Exception as gold_error:
                                print(f"âŒ [EVAL] Gold SQL ì²˜ë¦¬ ì‹¤íŒ¨: {gold_error}")
                                eval_result['exact_match'] = False
                                eval_result['execution_match'] = False

                    except Exception as parsing_error:
                        print(f"âŒ [EVAL] SQL íŒŒì‹± ì‹¤íŒ¨: {parsing_error}")
                        eval_result['syntax_correct'] = False
                        eval_result['parsing_error'] = str(parsing_error)
                else:
                    print(f"âš ï¸ [EVAL] ìƒì„±ëœ SQLì´ ì—†ìŒ")
                    eval_result['syntax_correct'] = False

                # ğŸ”¥ ë¬¸ì œ 6: í‰ê°€ ê²°ê³¼ ì €ì¥
                try:
                    self.save_evaluation_result(eval_result)
                    print(f"âœ… [EVAL] í‰ê°€ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")
                except Exception as save_eval_error:
                    print(f"âš ï¸ [EVAL] í‰ê°€ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {save_eval_error}")

                # ğŸ”¥ ë¬¸ì œ 7: ì¶œë ¥ í•¨ìˆ˜ë“¤ - ì•ˆì „í•˜ê²Œ í˜¸ì¶œ
                try:
                    # ë©€í‹°í„´ ê°œë³„ í‰ê°€ ê²°ê³¼ ì¶œë ¥ (ì½˜ì†”ìš©)
                    if hasattr(self, 'print_individual_evaluation'):
                        self.print_individual_evaluation(eval_result)
                        print(f"âœ… [EVAL] ê°œë³„ í‰ê°€ ì¶œë ¥ ì™„ë£Œ")
                except Exception as individual_error:
                    print(f"âš ï¸ [EVAL] ê°œë³„ í‰ê°€ ì¶œë ¥ ì‹¤íŒ¨: {individual_error}")

                try:
                    # ì „ì²´ í‰ê°€ í†µê³„ ì¶œë ¥ (ì½˜ì†”ìš©)
                    if hasattr(self, 'print_aggregate_evaluation'):
                        self.print_aggregate_evaluation()
                        print(f"âœ… [EVAL] ì „ì²´ í‰ê°€ ì¶œë ¥ ì™„ë£Œ")
                except Exception as aggregate_error:
                    print(f"âš ï¸ [EVAL] ì „ì²´ í‰ê°€ ì¶œë ¥ ì‹¤íŒ¨: {aggregate_error}")

                print(f"ğŸ‰ [EVAL] ì „ì²´ í‰ê°€ ê³¼ì • ì™„ë£Œ")
                return eval_result

            except Exception as eval_error:
                print(f"âŒ [EVAL] ê¸°ë³¸ í‰ê°€ ìˆ˜í–‰ ì‹¤íŒ¨: {eval_error}")
                import traceback
                traceback.print_exc()

                # ğŸ”¥ ê°œì„ : í‰ê°€ ì‹¤íŒ¨ ì‹œì—ë„ ê¸°ë³¸ ê²°ê³¼ ë°˜í™˜
                return {
                    "user_question": user_question,
                    "generated_sql": generated_sql,
                    "gold_sql": gold_sql or "",
                    "eval_error": str(eval_error),
                    "timestamp": datetime.now().isoformat(),
                    "syntax_correct": False,
                    "execution_success": exec_success,
                    "result_count": result_count
                }

        except Exception as overall_error:
            print(f"âŒ [EVAL] ì „ì²´ ê³¼ì • ì‹¤íŒ¨: {overall_error}")
            import traceback
            traceback.print_exc()

            # ìµœì¢… ì•ˆì „ë§ - ìµœì†Œí•œì˜ ê²°ê³¼ë¼ë„ ë°˜í™˜
            return {
                "user_question": user_question,
                "generated_sql": generated_sql,
                "overall_error": str(overall_error),
                "timestamp": datetime.now().isoformat()
            }

    def save_evaluation_result(self, eval_result):
        """í‰ê°€ ê²°ê³¼ë¥¼ íŒŒì¼ì— ì €ì¥"""
        try:
            # ê¸°ì¡´ í‰ê°€ ê²°ê³¼ë“¤ ë¡œë“œ
            if os.path.exists(self.evaluation_file):
                with open(self.evaluation_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                if isinstance(data, list):
                    evaluations = data
                else:
                    evaluations = data.get("evaluations", [])
            else:
                evaluations = []

            # ìƒˆ í‰ê°€ ê²°ê³¼ ì¶”ê°€
            evaluations.append(eval_result)

            # ìµœê·¼ 1000ê°œë§Œ ìœ ì§€
            if len(evaluations) > 1000:
                evaluations = evaluations[-1000:]

            # íŒŒì¼ì— ì €ì¥
            with open(self.evaluation_file, 'w', encoding='utf-8') as f:
                json.dump({"evaluations": evaluations}, f, indent=2, ensure_ascii=False)

        except Exception as e:
            print(f"âŒ í‰ê°€ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")

    def calculate_aggregate_scores(self):
        # ì „ì²´ í‰ê°€ í†µê³„ ê³„ì‚° (ê¸°ì¡´ ì½”ë“œ ìœ ì§€, ë³´ì¡° í•¨ìˆ˜ë¡œ í™œìš©)
        try:
            if not os.path.exists(self.evaluation_file):
                return None

            with open(self.evaluation_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            if isinstance(data, list):
                results = data
            elif isinstance(data, dict):
                results = data.get("evaluations", [])
            else:
                print("âŒ í‰ê°€ íŒŒì¼ í˜•ì‹ ì˜¤ë¥˜")
                return None

            if not results:
                return None

            # ê¸°ë³¸ í†µê³„ ì´ˆê¸°í™”
            total_count = len(results)
            exact_match_count = sum(1 for r in results if r.get("exact_match", False))
            exact_match_rate = (exact_match_count / total_count) * 100 if total_count > 0 else 0

            # í†µê³„ ì •ë³´ ë°˜í™˜
            return {
                "total_count": total_count,
                "exact_match_rate": exact_match_rate,
                "results": results
            }

        except Exception as e:
            print(f"âŒ ì „ì²´ í‰ê°€ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return None


def get_difficulty_from_sql(self, sql_string):
    """SQL ë‚œì´ë„ íŒì • - ROWNUM ì œì™¸í•˜ì—¬ ìˆœìˆ˜ SQL ë³µì¡ë„ë¡œ í‰ê°€"""
    try:
        if not sql_string or sql_string.strip() == '':
            return "Easy"

        # ğŸ”¥ í•µì‹¬ ìˆ˜ì •: ë‚œì´ë„ ë¶„ë¥˜ìš©ìœ¼ë¡œ ROWNUM ì œê±°
        # ì„±ëŠ¥ìš© ROWNUMì„ ì œê±°í•˜ê³  ìˆœìˆ˜ SQL ë…¼ë¦¬ë¡œë§Œ ë‚œì´ë„ í‰ê°€
        clean_sql = self._remove_rownum_for_difficulty_analysis(sql_string)

        # ì •ê·œí™”ëœ SQLë¡œ íŒŒì‹±
        normalized_sql = normalize_oracle_sql_for_comparison(clean_sql)
        parsed_sql = get_sql(self.sql_evaluator.schema, normalized_sql)

        if self.sql_evaluator.evaluator:
            hardness = self.sql_evaluator.evaluator.eval_hardness(parsed_sql)
            return hardness.capitalize()
        else:
            return self._calculate_hardness_direct(parsed_sql)

    except Exception as e:
        print(f"âŒ SQL ë‚œì´ë„ íŒì • ì˜¤ë¥˜: {e}")
        return "Easy"


def _remove_rownum_for_difficulty_analysis(self, sql_string):
    """ë‚œì´ë„ ë¶„ì„ìš©ìœ¼ë¡œ ROWNUM ê´€ë ¨ ì¡°ê±´ì„ ì œê±°"""
    try:
        sql = sql_string.strip()

        # 1. ë‹¨ìˆœí•œ WHERE rownum <= N íŒ¨í„´ ì œê±°
        sql = re.sub(r'\s+WHERE\s+rownum\s*<=\s*\d+\s*$', '', sql, flags=re.IGNORECASE)
        sql = re.sub(r'\s+AND\s+rownum\s*<=\s*\d+', '', sql, flags=re.IGNORECASE)
        sql = re.sub(r'rownum\s*<=\s*\d+\s+AND\s+', '', sql, flags=re.IGNORECASE)

        # 2. ì„œë¸Œì¿¼ë¦¬ ì™¸ë¶€ì˜ WHERE rownum <= N ì œê±°
        # SELECT * FROM (...) WHERE rownum <= 100 íŒ¨í„´
        sql = re.sub(r'\)\s+WHERE\s+rownum\s*<=\s*\d+\s*$', ')', sql, flags=re.IGNORECASE)

        # 3. ë³µì¡í•œ WHERE ì ˆì—ì„œ rownum ì¡°ê±´ë§Œ ì œê±°
        # WHERE condition1 AND rownum <= 100 AND condition2 ê°™ì€ ê²½ìš°
        sql = re.sub(r'\s+AND\s+rownum\s*<=\s*\d+\s+AND\s+', ' AND ', sql, flags=re.IGNORECASE)

        # 4. WHERE ì ˆì´ rownumë§Œ ìˆì—ˆë˜ ê²½ìš° WHERE ìì²´ ì œê±°
        sql = re.sub(r'\s+WHERE\s*$', '', sql, flags=re.IGNORECASE)

        # 5. ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
        sql = re.sub(r'\s+', ' ', sql).strip()

        return sql

    except Exception as e:
        print(f"âŒ ROWNUM ì œê±° ì¤‘ ì˜¤ë¥˜: {e}")
        return sql_string


def _calculate_hardness_direct(self, parsed_sql):
    """ì§ì ‘ ë‚œì´ë„ ê³„ì‚° (Evaluator ì—†ì„ ë•Œ ë°±ì—…)"""
    try:
        component1_count = count_component1(parsed_sql)
        component2_count = count_component2(parsed_sql)
        others_count = count_others(parsed_sql)

        if component1_count <= 1 and others_count == 0 and component2_count == 0:
            return "Easy"
        elif (others_count <= 2 and component1_count <= 1 and component2_count == 0) or \
                (component1_count <= 2 and others_count < 2 and component2_count == 0):
            return "Medium"
        elif (others_count <= 2 and component1_count <= 2 and component2_count <= 1) or \
                (component1_count <= 3 and others_count <= 2 and component2_count == 0) or \
                (component1_count <= 1 and others_count == 0 and component2_count <= 1):
            return "Hard"
        else:
            return "Extra"

    except Exception as e:
        print(f"âŒ ì§ì ‘ ë‚œì´ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
        return "Easy"


# === [6] ë©€í‹°í„´ ì§‘ê³„ í‰ê°€ ê´€ë¦¬ì ===
class MultiTurnAggregateEvaluationManager:
    """ë©€í‹°í„´ í‰ê°€ ê²°ê³¼ ì§‘ê³„ ë° ë¶„ì„ ê´€ë¦¬ì"""

    def __init__(self, sql_evaluator):
        self.sql_evaluator = sql_evaluator
        self.session_file = "multiturn_sessions.json"

    def generate_multiturn_evaluation_report(self):
        """ë©€í‹°í„´ í‰ê°€ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±"""
        try:
            if not os.path.exists(self.session_file):
                return "ğŸ“‹ ë©€í‹°í„´ ì„¸ì…˜ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."

            with open(self.session_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            sessions = data.get('multiturn_sessions', [])
            if not sessions:
                return "ğŸ“‹ ì €ì¥ëœ ë©€í‹°í„´ ì„¸ì…˜ì´ ì—†ìŠµë‹ˆë‹¤."

            # í˜„ì¬ í™œì„± ì„¸ì…˜ ì •ë³´
            current_session_info = ""
            if (hasattr(self.sql_evaluator, 'multiturn_manager') and
                    self.sql_evaluator.multiturn_manager and
                    self.sql_evaluator.multiturn_manager.current_session):
                current = self.sql_evaluator.multiturn_manager.current_session
                current_session_info = f"""
ğŸ¯ **í˜„ì¬ í™œì„± ì„¸ì…˜**
- ì„¸ì…˜ ID: {current.session_id}
- ìƒíƒœ: {current.status}
- ì§„í–‰ í„´: {len(current.turns)}/{current.max_turns}
- í† í° ì‚¬ìš©ëŸ‰: {current.total_tokens:,} tokens

"""

            # ì§‘ê³„ ë¶„ì„
            report = f"""{current_session_info}ğŸ“Š **ë©€í‹°í„´ í‰ê°€ ì¢…í•© ë¦¬í¬íŠ¸**

ğŸ”¢ **ê¸°ë³¸ í†µê³„**
- ì´ ì„¸ì…˜ ìˆ˜: {len(sessions)}
- ì™„ë£Œëœ ì„¸ì…˜: {sum(1 for s in sessions if s.get('status') == 'ì™„ë£Œ')}
- ì§„í–‰ì¤‘ì¸ ì„¸ì…˜: {sum(1 for s in sessions if s.get('status') == 'ì§„í–‰ì¤‘')}

"""

            # ì •í™•ë„ í†µê³„
            exact_match_stats = self._calculate_exact_match_accuracy(sessions)
            execution_stats = self._calculate_execution_accuracy(sessions)

            if isinstance(exact_match_stats, dict) and 'all' in exact_match_stats:
                report += f"""ğŸ¯ **ì •í™•ë„ í†µê³„**
- ì „ì²´ Exact Match: {exact_match_stats['all']:.1%}
- ì „ì²´ Execution Match: {execution_stats.get('all', 0):.1%}

"""

            # í„´ë³„ ì„±ëŠ¥
            clause_progress = self._calculate_clause_progress_by_turn(sessions)
            if clause_progress:
                report += "ğŸ“ˆ **ì ˆë³„ ì§„í–‰ë„ (í„´ë³„ í‰ê· )**\n"
                for turn_num, scores in clause_progress.items():
                    if turn_num != 'final' and scores:
                        valid_scores = [v for v in scores.values() if v is not None]
                        if valid_scores:
                            avg_score = sum(valid_scores) / len(valid_scores)
                            report += f"- Turn {turn_num}: {avg_score:.1%}\n"

            # ìµœì¢… ì„±ëŠ¥ (ë§ˆì§€ë§‰ í„´)
            if 'final' in clause_progress:
                final_scores = clause_progress['final']
                valid_final = [v for v in final_scores.values() if v is not None]
                if valid_final:
                    final_avg = sum(valid_final) / len(valid_final)
                    report += f"\nğŸ† **ìµœì¢… ì„±ëŠ¥** (ë§ˆì§€ë§‰ í„´): {final_avg:.1%}\n"

            # ìµœê·¼ 3ê°œ ì„¸ì…˜ ìš”ì•½
            recent_sessions = sessions[-3:]
            report += f"\nğŸ“ **ìµœê·¼ ì„¸ì…˜ë“¤**\n"
            for session in recent_sessions:
                session_id = session.get('session_id', 'Unknown')
                status = session.get('status', 'Unknown')
                turns_count = len(session.get('turns', []))
                max_turns = session.get('max_turns', 5)
                tokens = session.get('total_tokens', 0)

                report += f"- {session_id}: {status} ({turns_count}/{max_turns} í„´, {tokens:,} tokens)\n"

            return report

        except Exception as e:
            return f"âŒ ë©€í‹°í„´ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}"

    def _calculate_clause_progress_by_turn(self, sessions):
        """í„´ë³„ ì ˆ ì§„í–‰ë„ ê³„ì‚°"""
        try:
            if not sessions:
                return {}

            # ëª¨ë“  ì„¸ì…˜ì˜ ìµœëŒ€ í„´ ìˆ˜ ì°¾ê¸°
            session_turn_counts = [len(s.get('turns', [])) for s in sessions if s.get('turns')]
            if not session_turn_counts:
                return {}

            max_turns = max(session_turn_counts)
            result = {}

            # í„´ë³„ ê³„ì‚°
            for turn_num in range(1, max_turns + 1):
                clause_data = {clause: [] for clause in STANDARD_CLAUSES}

                for session in sessions:
                    turns = session.get('turns', [])
                    if len(turns) >= turn_num:
                        turn = turns[turn_num - 1]
                        clause_progress = turn.get('clause_progress', {})
                        for clause in STANDARD_CLAUSES:
                            score = clause_progress.get(clause, None)
                            clause_data[clause].append(score)

                # í‰ê·  ê³„ì‚°
                result[turn_num] = {}
                for clause in STANDARD_CLAUSES:
                    scores = clause_data[clause]
                    if scores:  # ìœ íš¨í•œ ì ìˆ˜ê°€ ìˆëŠ” ê²½ìš°
                        result[turn_num][clause] = sum(scores) / len(scores)
                    else:  # ëª¨ë“  ê°’ì´ Noneì¸ ê²½ìš°
                        result[turn_num][clause] = None

            # Final ê³„ì‚° (ëª¨ë“  ì„¸ì…˜ì˜ ë§ˆì§€ë§‰ í„´)
            final_scores = {clause: [] for clause in STANDARD_CLAUSES}
            for session in sessions:
                turns = session.get('turns', [])
                if turns:
                    last_turn = turns[-1]
                    clause_progress = last_turn.get('clause_progress', {})
                    for clause in STANDARD_CLAUSES:
                        score = clause_progress.get(clause, None)
                        if score is not None:
                            final_scores[clause].append(score)

            # Final í‰ê·  ê³„ì‚°
            result['final'] = {}
            for clause in STANDARD_CLAUSES:
                scores = final_scores[clause]
                if scores:
                    result['final'][clause] = sum(scores) / len(scores)
                else:
                    result['final'][clause] = None

            return result

        except Exception as e:
            print(f"âŒ ì ˆë³„ ì§„í–‰ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}

    def _calculate_execution_accuracy(self, sessions):
        """ì‹¤í–‰ ì •í™•ë„ ê³„ì‚°"""
        stats = {}

        if not sessions:
            return {'all': 0.0}

        # ë¹ˆ í„´ì´ ìˆëŠ” ì„¸ì…˜ í•„í„°ë§
        valid_sessions = [s for s in sessions if s.get('turns')]
        if not valid_sessions:
            return {'all': 0.0}

        session_turn_counts = [len(s.get('turns', [])) for s in valid_sessions]
        max_turns = max(session_turn_counts) if session_turn_counts else 5

        # í„´ë³„ ê³„ì‚°
        for turn_num in range(1, max_turns + 1):
            success_count = 0
            total_count = 0

            for session in sessions:
                turns = session.get('turns', [])
                if len(turns) >= turn_num:
                    total_count += 1
                    if turns[turn_num - 1].get('execution_match', False):
                        success_count += 1

            stats[turn_num] = {
                'rate': success_count / total_count if total_count > 0 else 0.0,
                'success': success_count,
                'total': total_count
            }

        # All ê³„ì‚°
        all_success = sum([s['success'] for s in stats.values()])
        all_total = sum([s['total'] for s in stats.values()])
        stats['all'] = all_success / all_total if all_total > 0 else 0.0

        return stats

    def _calculate_exact_match_accuracy(self, sessions):
        """ì •í™• ì¼ì¹˜ ì •í™•ë„ ê³„ì‚°"""
        stats = {}

        if not sessions:
            return {'all': 0.0}

        valid_sessions = [s for s in sessions if s.get('turns')]
        if not valid_sessions:
            return {'all': 0.0}

        session_turn_counts = [len(s.get('turns', [])) for s in valid_sessions]
        max_turns = max(session_turn_counts) if session_turn_counts else 5

        # í„´ë³„ ê³„ì‚°
        for turn_num in range(1, max_turns + 1):
            success_count = 0
            total_count = 0

            for session in sessions:
                turns = session.get('turns', [])
                if len(turns) >= turn_num:
                    total_count += 1
                    if turns[turn_num - 1].get('exact_match', False):
                        success_count += 1

            stats[turn_num] = {
                'rate': success_count / total_count if total_count > 0 else 0.0,
                'success': success_count,
                'total': total_count
            }

        # All ê³„ì‚°
        all_success = sum([s['success'] for s in stats.values()])
        all_total = sum([s['total'] for s in stats.values()])
        stats['all'] = all_success / all_total if all_total > 0 else 0.0

        return stats

    def _format_individual_evaluation_report(self, session):
        """ê°œë³„ ì„¸ì…˜ í‰ê°€ ë¦¬í¬íŠ¸ í˜•ì‹í™”"""
        try:
            session_id = session.get('session_id', 'Unknown')
            status = session.get('status', 'Unknown')
            turns = session.get('turns', [])
            max_turns = session.get('max_turns', 5)
            total_tokens = session.get('total_tokens', 0)

            report = f"""ğŸ“‹ **ê°œë³„ ì„¸ì…˜ í‰ê°€: {session_id}**

ğŸ¯ **ì„¸ì…˜ ì •ë³´**
- ìƒíƒœ: {status}
- ì§„í–‰ í„´: {len(turns)}/{max_turns}
- ì´ í† í° ì‚¬ìš©ëŸ‰: {total_tokens:,} tokens

"""

            if turns:
                report += "ğŸ“Š **í„´ë³„ ê²°ê³¼**\n"
                for i, turn in enumerate(turns, 1):
                    question = turn.get('user_question', '')[:50]
                    exact_match = turn.get('exact_match', False)
                    execution_match = turn.get('execution_match', False)

                    exact_icon = "âœ…" if exact_match else "âŒ"
                    exec_icon = "âœ…" if execution_match else "âŒ"

                    report += f"Turn {i}: {question}... (Exact: {exact_icon}, Exec: {exec_icon})\n"

            return report

        except Exception as e:
            return f"âŒ ê°œë³„ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}"


# === [7] ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ===
sql_evaluator = SQLEvaluationModule()


# === [8] í˜¸í™˜ì„± í•¨ìˆ˜ë“¤ ===
def evaluate_and_save(user_question, generated_sql, gold_sql=None, exec_success=False, result_count=0):
    """ì „ì—­ í‰ê°€ í•¨ìˆ˜ (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)"""
    return sql_evaluator.evaluate_and_save(
        user_question=user_question,
        generated_sql=generated_sql,
        gold_sql=gold_sql,
        exec_success=exec_success,
        result_count=result_count
    )


def get_query_stats():
    """ì¿¼ë¦¬ í†µê³„ ì¡°íšŒ (í•˜ìœ„ í˜¸í™˜ì„±)"""
    return sql_evaluator.calculate_aggregate_scores()


def start_multiturn_session(max_turns=5):
    """ë©€í‹°í„´ ì„¸ì…˜ ì‹œì‘ (í•˜ìœ„ í˜¸í™˜ì„±)"""
    if sql_evaluator.multiturn_manager:
        return sql_evaluator.multiturn_manager.start_new_session(max_turns)
    else:
        print("âŒ ë©€í‹°í„´ ê´€ë¦¬ìê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None


def get_individual_evaluation_result():
    """ê°œë³„ í‰ê°€ ê²°ê³¼ ì¡°íšŒ (í•˜ìœ„ í˜¸í™˜ì„±)"""
    if (sql_evaluator.multiturn_manager and
            hasattr(sql_evaluator.multiturn_manager, 'current_session') and
            sql_evaluator.multiturn_manager.current_session):

        aggregate_manager = MultiTurnAggregateEvaluationManager(sql_evaluator)
        return aggregate_manager._format_individual_evaluation_report(
            sql_evaluator.multiturn_manager.current_session.to_dict()
        )
    else:
        return "ğŸ“‹ í™œì„±í™”ëœ ë©€í‹°í„´ ì„¸ì…˜ì´ ì—†ìŠµë‹ˆë‹¤."


def evaluate_new_rag_metrics(user_question, generated_sql, context_quality=0.8, relevance_score=0.9):
    """RAG ë©”íŠ¸ë¦­ í‰ê°€ (í•˜ìœ„ í˜¸í™˜ì„±)"""
    rag_evaluation = {
        "context_quality": context_quality,
        "relevance_score": relevance_score,
        "user_question_length": len(user_question),
        "generated_sql_length": len(generated_sql) if generated_sql else 0,
        "evaluation_timestamp": datetime.now().isoformat()
    }

    # ì „ì—­ í‰ê°€ìì˜ RAG í‰ê°€ ê²°ê³¼ì— ì €ì¥
    sql_evaluator.last_rag_evaluation = rag_evaluation
    return rag_evaluation


def evaluate_langsmith_rag_metrics(context_data, retrieval_quality=0.85):
    """LangSmith RAG ë©”íŠ¸ë¦­ í‰ê°€ (í•˜ìœ„ í˜¸í™˜ì„±)"""
    langsmith_evaluation = {
        "retrieval_quality": retrieval_quality,
        "context_data_size": len(str(context_data)) if context_data else 0,
        "langsmith_timestamp": datetime.now().isoformat()
    }

    # ê¸°ì¡´ RAG í‰ê°€ì™€ ë³‘í•©
    if hasattr(sql_evaluator, 'last_rag_evaluation'):
        sql_evaluator.last_rag_evaluation.update(langsmith_evaluation)
    else:
        sql_evaluator.last_rag_evaluation = langsmith_evaluation

    return langsmith_evaluation


def get_latest_aggregate_result():
    """ìµœì‹  ì§‘ê³„ ê²°ê³¼ ì¡°íšŒ (í•˜ìœ„ í˜¸í™˜ì„±)"""
    if sql_evaluator.multiturn_manager:
        aggregate_manager = MultiTurnAggregateEvaluationManager(sql_evaluator)
        return aggregate_manager.generate_multiturn_evaluation_report()
    else:
        return getattr(sql_evaluator, 'last_aggregate_result', 'ì „ì²´ í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.')


# ëª¨ë“ˆ ë¡œë”© ì™„ë£Œ ë©”ì‹œì§€
print("ğŸ‰ SParC ê³µì‹ ë¡œì§ ì ìš© ë©€í‹°í„´ í‰ê°€ ëª¨ë“ˆ v2 ë¡œë”© ì™„ë£Œ!")