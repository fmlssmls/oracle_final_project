"""
embed_schema_hybrid.py - í•˜ì´ë¸Œë¦¬ë“œ ì²­í‚¹ ë°©ì‹
ê¸°ì¡´ ìŠ¤í‚¤ë§ˆ + ìƒì„¸ ìŠ¤í‚¤ë§ˆ í†µí•© ì²˜ë¦¬

ì£¼ìš” íŠ¹ì§•:
1. ê¸°ì¡´ ìŠ¤í‚¤ë§ˆ: í…Œì´ë¸”ë³„ ë…ë¦½ ì²­í¬ + FAQ ì²­í¬
2. ìƒì„¸ ìŠ¤í‚¤ë§ˆ: í—¤ë” ê¸°ë°˜ ì˜ë¯¸ ë‹¨ìœ„ ì²­í¬
3. ê´€ê³„ì •ë³´ ì²­í¬ (ë³µì¡í•œ JOIN ì²˜ë¦¬)
4. ë„ë©”ì¸ë³„ ë©”íƒ€ë°ì´í„° (ì˜ë£Œ ë„ë©”ì¸ íŠ¹ì„± ë°˜ì˜)
"""

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document
import re
import os
import json
from datetime import datetime


class HybridSchemaChunker:
    """í•˜ì´ë¸Œë¦¬ë“œ ì²­í‚¹ì„ ìœ„í•œ ë©”ì¸ í´ë˜ìŠ¤"""

    def __init__(self):
        # ëª¨ë“  ìŠ¤í‚¤ë§ˆ íŒŒì¼ ì •ì˜ (ê¸°ì¡´ + ìƒì„¸)
        self.schema_files = [
            # ê¸°ì¡´ ìŠ¤í‚¤ë§ˆ íŒŒì¼ë“¤
            'schema_patients.txt',
            'schema_diagproc.txt',
            'schema_drugs.txt',
            'schema_events.txt',
            'schema_trial.txt',
            # ìƒˆë¡œìš´ ìƒì„¸ ìŠ¤í‚¤ë§ˆ íŒŒì¼ë“¤
            'schema_patients_detailed.txt',
            'schema_events_detailed.txt',
            'schema_diagproc_detailed.txt',
            'schema_drugs_detailed.txt',
            'schema_trial_detailed.txt'
        ]

        # ë„ë©”ì¸ë³„ ë¶„ë¥˜ (ì˜ë£Œ ì›Œí¬í”Œë¡œìš° ìˆœì„œ)
        self.domain_map = {
            'patients': {'domain': 'í™˜ìê¸°ë³¸ì •ë³´', 'priority': 1, 'keywords': ['í™˜ì', 'ë‚˜ì´', 'ì„±ë³„', 'ì…ì›', 'í‡´ì›']},
            'diagproc': {'domain': 'ì§„ë‹¨ì‹œìˆ ', 'priority': 2, 'keywords': ['ì§„ë‹¨', 'ì§ˆë³‘', 'ICD', 'ì‹œìˆ ', 'ìˆ˜ìˆ ']},
            'drugs': {'domain': 'ì•½ë¬¼ì¹˜ë£Œ', 'priority': 3, 'keywords': ['ì•½ë¬¼', 'ì²˜ë°©', 'íˆ¬ì•½', 'ìš©ëŸ‰', 'í•­ìƒì œ']},
            'events': {'domain': 'ì„ìƒì´ë²¤íŠ¸', 'priority': 4, 'keywords': ['ê²€ì‚¬', 'ìˆ˜ì¹˜', 'ì¸¡ì •', 'ëª¨ë‹ˆí„°ë§']},
            'trial': {'domain': 'ì„ìƒì‹œí—˜', 'priority': 5, 'keywords': ['ì‹œí—˜', 'ì—°êµ¬', 'ì„ìƒ', 'ì¹˜ë£Œíš¨ê³¼']}
        }

        self.all_chunks = []  # ëª¨ë“  ì²­í¬ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸

    def extract_table_info(self, content, source_file):
        """ê¸°ì¡´ ìŠ¤í‚¤ë§ˆ: í…Œì´ë¸” ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ê°œë³„ ì²­í¬ë¡œ ìƒì„±"""
        chunks = []

        # ì‹¤ì œ íŒŒì¼ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •: ëŒ€ë¬¸ì í…Œì´ë¸”ëª…ë§Œ ë‹¨ë…ìœ¼ë¡œ ìˆëŠ” íŒ¨í„´
        table_sections = re.split(r'\n(?=[A-Z_]+\n)', content)

        for section in table_sections:
            section = section.strip()
            if len(section) < 50:  # ë„ˆë¬´ ì§§ì€ ì„¹ì…˜ ì œì™¸
                continue

            # í…Œì´ë¸”ëª… ì¶”ì¶œ: ì²« ë²ˆì§¸ ì¤„ì—ì„œ ëŒ€ë¬¸ì ë‹¨ì–´ ì°¾ê¸°
            lines = section.split('\n')
            table_name = None

            for line in lines:
                line = line.strip()
                # ëŒ€ë¬¸ìë¡œë§Œ ì´ë£¨ì–´ì§€ê³ , |ê°€ ì—†ê³ , #ìœ¼ë¡œ ì‹œì‘í•˜ì§€ ì•ŠëŠ” ë‹¨ë… ë‹¨ì–´
                if (line.isupper() and
                    not '|' in line and
                    not line.startswith('#') and
                    not line.startswith('[') and
                    len(line.split()) == 1 and
                    (line.isalpha() or '_' in line)):
                    table_name = line.lower()
                    break

            if not table_name:
                continue

            # ë„ë©”ì¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            file_key = source_file.replace('schema_', '').replace('.txt', '').replace('_detailed', '')
            domain_info = self.domain_map.get(file_key, {})

            # ì»¬ëŸ¼ ì •ë³´ ì •ë¦¬ (| êµ¬ë¶„ì ì‚¬ìš©)
            columns = []
            for line in lines:
                if '|' in line and not line.strip().startswith('#'):
                    col_parts = line.split('|')
                    if len(col_parts) >= 2:
                        col_name = col_parts[0].strip()
                        col_desc = col_parts[1].strip()
                        # ë¹ˆ ê°’ì´ ì•„ë‹ˆê³  í…Œì´ë¸”ëª…ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
                        if col_name and col_desc and col_name != table_name.upper():
                            columns.append(f"{col_name}: {col_desc}")

            # ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ
            if not columns:
                continue

            # í…Œì´ë¸” ì²­í¬ ìƒì„±
            table_content = f"í…Œì´ë¸”: {table_name.upper()}\n"
            table_content += f"ë„ë©”ì¸: {domain_info.get('domain', 'ê¸°íƒ€')}\n"
            table_content += f"ì»¬ëŸ¼ ì •ë³´:\n" + "\n".join(columns[:15])  # ìµœëŒ€ 15ê°œ ì»¬ëŸ¼

            chunks.append(Document(
                page_content=table_content,
                metadata={
                    "type": "table_schema",  # ì²­í¬ íƒ€ì…
                    "source": source_file,
                    "table_name": table_name,
                    "domain": domain_info.get('domain', 'ê¸°íƒ€'),
                    "priority": domain_info.get('priority', 99),
                    "keywords": ", ".join(domain_info.get('keywords', [])),
                    "column_count": len(columns)
                }
            ))

        return chunks

    def extract_faq_info(self, content, source_file):
        """ê¸°ì¡´ ìŠ¤í‚¤ë§ˆ: FAQ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ê°œë³„ ì²­í¬ë¡œ ìƒì„±"""
        chunks = []

        # Q&A íŒ¨í„´ ë§¤ì¹­
        qa_pairs = re.findall(r"Q[:ï¼š]\s*(.*?)\nA[:ï¼š]\s*(.*?)(?=\n(?:Q[:ï¼š]|#|\d+\.|\Z))", content, re.DOTALL)

        file_key = source_file.replace('schema_', '').replace('.txt', '').replace('_detailed', '')
        domain_info = self.domain_map.get(file_key, {})

        for i, (question, answer) in enumerate(qa_pairs):
            q_clean = question.strip()
            a_clean = answer.strip()

            # ì˜ë¯¸ìˆëŠ” Q&Aë§Œ ì„ ë³„
            if len(q_clean) < 5 or len(a_clean) < 10:
                continue

            # FAQ ì²­í¬ ìƒì„±
            faq_content = f"Q: {q_clean}\nA: {a_clean}"

            # SQLì´ í¬í•¨ëœ ë‹µë³€ì¸ì§€ í™•ì¸
            has_sql = any(keyword in a_clean.upper() for keyword in ['SELECT', 'FROM', 'WHERE', 'JOIN'])

            chunks.append(Document(
                page_content=faq_content,
                metadata={
                    "type": "table_faq",  # ì²­í¬ íƒ€ì…
                    "source": source_file,
                    "domain": domain_info.get('domain', 'ê¸°íƒ€'),
                    "priority": domain_info.get('priority', 99),
                    "keywords": ", ".join(domain_info.get('keywords', [])),
                    "has_sql": has_sql,
                    "faq_id": i + 1
                }
            ))

        return chunks

    def extract_detailed_chunks(self, content, source_file):
        """ìƒì„¸ ìŠ¤í‚¤ë§ˆ: í—¤ë” ê¸°ë°˜ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ì²­í¬ ìƒì„±"""
        chunks = []

        # íŒŒì¼ í‚¤ì™€ ë„ë©”ì¸ ì •ë³´ ì¶”ì¶œ
        file_key = source_file.replace('schema_', '').replace('_detailed.txt', '')
        domain_info = self.domain_map.get(file_key, {})

        # 1. ë©”ì¸ ì„¹ì…˜ë³„ë¡œ ë¶„í•  (## í—¤ë” ê¸°ì¤€)
        main_sections = re.split(r'\n## ', content)

        for main_section in main_sections:
            main_section = main_section.strip()
            if len(main_section) < 100:
                continue

            # ë©”ì¸ ì„¹ì…˜ ì œëª© ì¶”ì¶œ
            section_title = main_section.split('\n')[0].strip()

            # 2. í•˜ìœ„ ì„¹ì…˜ìœ¼ë¡œ ë¶„í•  (### í—¤ë” ê¸°ì¤€)
            sub_sections = re.split(r'\n### ', main_section)

            if len(sub_sections) <= 1:
                # í•˜ìœ„ ì„¹ì…˜ì´ ì—†ìœ¼ë©´ ë©”ì¸ ì„¹ì…˜ì„ ê·¸ëŒ€ë¡œ ì²­í¬ë¡œ ìƒì„±
                chunks.append(Document(
                    page_content=main_section,
                    metadata={
                        "type": "detailed_section",
                        "source": source_file,
                        "section_title": section_title,
                        "domain": domain_info.get('domain', 'ê¸°íƒ€'),
                        "priority": domain_info.get('priority', 99),
                        "keywords": ", ".join(domain_info.get('keywords', []))
                    }
                ))
            else:
                # í•˜ìœ„ ì„¹ì…˜ì´ ìˆìœ¼ë©´ ê°ê°ì„ ì²­í¬ë¡œ ìƒì„±
                for sub_section in sub_sections:
                    sub_section = sub_section.strip()
                    if len(sub_section) < 50:
                        continue

                    # í•˜ìœ„ ì„¹ì…˜ ì œëª© ì¶”ì¶œ (í…Œì´ë¸”ëª… ë“±)
                    sub_title = sub_section.split('\n')[0].strip()

                    # í…Œì´ë¸”ëª… ì¶”ì¶œ ì‹œë„
                    table_match = re.search(r'^([A-Z_]+)', sub_title)
                    table_name = table_match.group(1).lower() if table_match else "unknown"

                    # 3. í° ì„¹ì…˜ì€ ë” ì„¸ë¶„í™” (**bold** í—¤ë” ê¸°ì¤€)
                    if len(sub_section) > 2000:
                        detail_parts = re.split(r'\n\*\*(.+?)\*\*', sub_section)

                        for i, part in enumerate(detail_parts):
                            part = part.strip()
                            if len(part) < 100:
                                continue

                            # ì§ìˆ˜ ì¸ë±ìŠ¤ëŠ” ì œëª©, í™€ìˆ˜ ì¸ë±ìŠ¤ëŠ” ë‚´ìš©
                            part_type = "detail_header" if i % 2 == 1 else "detail_content"

                            chunks.append(Document(
                                page_content=part,
                                metadata={
                                    "type": "detailed_subsection",
                                    "source": source_file,
                                    "section_title": section_title,
                                    "sub_title": sub_title,
                                    "table_name": table_name,
                                    "part_type": part_type,
                                    "domain": domain_info.get('domain', 'ê¸°íƒ€'),
                                    "priority": domain_info.get('priority', 99),
                                    "keywords": ", ".join(domain_info.get('keywords', []))
                                }
                            ))
                    else:
                        # ì‘ì€ ì„¹ì…˜ì€ ê·¸ëŒ€ë¡œ ì²­í¬ë¡œ ìƒì„±
                        chunks.append(Document(
                            page_content=sub_section,
                            metadata={
                                "type": "detailed_table",
                                "source": source_file,
                                "section_title": section_title,
                                "sub_title": sub_title,
                                "table_name": table_name,
                                "domain": domain_info.get('domain', 'ê¸°íƒ€'),
                                "priority": domain_info.get('priority', 99),
                                "keywords": ", ".join(domain_info.get('keywords', []))
                            }
                        ))

        return chunks

    def create_relationship_chunks(self):
        """í…Œì´ë¸” ê°„ ê´€ê³„ ì •ë³´ë¥¼ ì²­í¬ë¡œ ìƒì„±"""
        chunks = []

        # MIMIC-IV ê¸°ë°˜ ì£¼ìš” ê´€ê³„ ì •ì˜
        relationships = {
            "ê¸°ë³¸ì—°ê²°": {
                "content": """ì£¼ìš” í…Œì´ë¸” ì—°ê²° ê´€ê³„:
- PATIENTS.SUBJECT_ID â† ëª¨ë“  í…Œì´ë¸”ì˜ ê¸°ë³¸ í‚¤
- ADMISSIONS.HADM_ID â† ì…ì› ê´€ë ¨ í…Œì´ë¸” ì—°ê²°
- ICUSTAYS.STAY_ID â† ICU ê´€ë ¨ í…Œì´ë¸” ì—°ê²°
- í™˜ì â†’ ì…ì› â†’ ì§„ë‹¨/ì•½ë¬¼/ì´ë²¤íŠ¸ ìˆœì„œë¡œ ì—°ê²°""",
                "keywords": ["ì—°ê²°", "ê´€ê³„", "ì¡°ì¸", "í‚¤"]
            },
            "ì§„ë‹¨ê´€ê³„": {
                "content": """ì§„ë‹¨ ê´€ë ¨ í…Œì´ë¸” ì—°ê²°:
- DIAGNOSES_ICD â†” D_ICD_DIAGNOSES (ICD_CODEë¡œ ì—°ê²°)
- PROCEDURES_ICD â†” D_ICD_PROCEDURES (ICD_CODEë¡œ ì—°ê²°)
- ì§„ë‹¨ì½”ë“œì™€ ì§„ë‹¨ëª…ì„ ë§¤ì¹­í•  ë•Œ ì‚¬ìš©""",
                "keywords": ["ì§„ë‹¨", "ICD", "ì§ˆë³‘ì½”ë“œ"]
            },
            "ì•½ë¬¼ê´€ê³„": {
                "content": """ì•½ë¬¼ ê´€ë ¨ í…Œì´ë¸” ì—°ê²°:
- PRESCRIPTIONS â†” D_ITEMS (ITEMIDë¡œ ì—°ê²°)
- INPUTEVENTS â†” D_ITEMS (ITEMIDë¡œ ì—°ê²°)
- ì•½ë¬¼ì½”ë“œì™€ ì•½ë¬¼ëª…ì„ ë§¤ì¹­í•  ë•Œ ì‚¬ìš©""",
                "keywords": ["ì•½ë¬¼", "ì²˜ë°©", "íˆ¬ì•½", "ITEMID"]
            },
            "ê²€ì‚¬ê´€ê³„": {
                "content": """ê²€ì‚¬ ê´€ë ¨ í…Œì´ë¸” ì—°ê²°:
- LABEVENTS â†” D_LABITEMS (ITEMIDë¡œ ì—°ê²°)
- CHARTEVENTS â†” D_ITEMS (ITEMIDë¡œ ì—°ê²°)
- ê²€ì‚¬ì½”ë“œì™€ ê²€ì‚¬ëª…ì„ ë§¤ì¹­í•  ë•Œ ì‚¬ìš©""",
                "keywords": ["ê²€ì‚¬", "ì¸¡ì •", "ITEMID", "ê²°ê³¼"]
            }
        }

        for rel_name, rel_info in relationships.items():
            chunks.append(Document(
                page_content=rel_info["content"],
                metadata={
                    "type": "relationship",  # ì²­í¬ íƒ€ì…
                    "source": "system_generated",
                    "relationship_name": rel_name,
                    "keywords": ", ".join(rel_info["keywords"]),
                    "priority": 1  # ê´€ê³„ ì •ë³´ëŠ” ë†’ì€ ìš°ì„ ìˆœìœ„
                }
            ))

        return chunks

    def create_domain_guide_chunks(self):
        """ë„ë©”ì¸ë³„ ë¶„ì„ ê°€ì´ë“œ ì²­í¬ ìƒì„±"""
        chunks = []

        domain_guides = {
            "í™˜ìë¶„ì„ê°€ì´ë“œ": {
                "content": """í™˜ì ì •ë³´ ë¶„ì„ ì‹œ ì£¼ìš” í¬ì¸íŠ¸:
- ë‚˜ì´ëŒ€ë³„ ë¶„ì„: ANCHOR_AGE ì»¬ëŸ¼ í™œìš©
- ì„±ë³„ë³„ ë¶„ì„: GENDER ì»¬ëŸ¼ í™œìš©  
- ì…ì›ê¸°ê°„ ë¶„ì„: ADMITTIME, DISCHTIME í™œìš©
- ì‚¬ë§ ì—¬ë¶€: DOD (Date of Death) í™•ì¸
- ICU ì²´ë¥˜: ICUSTAYS í…Œì´ë¸”ê³¼ ì¡°ì¸""",
                "keywords": ["í™˜ì", "ë‚˜ì´", "ì„±ë³„", "ì…ì›", "ì‚¬ë§"]
            },
            "ì„ìƒë¶„ì„ê°€ì´ë“œ": {
                "content": """ì„ìƒ ë°ì´í„° ë¶„ì„ ì‹œ ì£¼ì˜ì‚¬í•­:
- ì‹œê°„ìˆœ ë¶„ì„: CHARTTIME ê¸°ì¤€ ì •ë ¬
- ì •ìƒë²”ìœ„ í™•ì¸: VALUENUMê³¼ REF_RANGE ë¹„êµ
- ê²°ì¸¡ê°’ ì²˜ë¦¬: NULL ê°’ì´ ë§ì€ í•­ëª© ì£¼ì˜
- ì¤‘ë³µ ì¸¡ì •: ê°™ì€ ì‹œê°„ëŒ€ ì¤‘ë³µ ì¸¡ì •ê°’ í™•ì¸
- ë‹¨ìœ„ í†µì¼: VALUEUOM í™•ì¸ í•„ìˆ˜""",
                "keywords": ["ì„ìƒ", "ì¸¡ì •", "ì‹œê°„", "ì •ìƒë²”ìœ„"]
            },
            "JOINê°€ì´ë“œ": {
                "content": """íš¨ê³¼ì ì¸ JOIN ì‚¬ìš©ë²•:
- í™˜ì ê¸°ë³¸ì •ë³´: PATIENTS í…Œì´ë¸”ì„ ì¤‘ì‹¬ìœ¼ë¡œ
- ì…ì›ë³„ ë¶„ì„: ADMISSIONS í…Œì´ë¸”ê³¼ ì¡°ì¸
- ICU ë¶„ì„: ICUSTAYS í…Œì´ë¸” í•„ìˆ˜
- ì§„ë‹¨ ì •ë³´: D_ICD_DIAGNOSESì™€ ì¡°ì¸ìœ¼ë¡œ ì§„ë‹¨ëª… í™•ì¸
- ì•½ë¬¼ ì •ë³´: D_ITEMSì™€ ì¡°ì¸ìœ¼ë¡œ ì•½ë¬¼ëª… í™•ì¸""",
                "keywords": ["JOIN", "ì¡°ì¸", "ì—°ê²°", "í…Œì´ë¸”"]
            }
        }

        for guide_name, guide_info in domain_guides.items():
            chunks.append(Document(
                page_content=guide_info["content"],
                metadata={
                    "type": "domain_guide",  # ì²­í¬ íƒ€ì…
                    "source": "system_generated",
                    "guide_name": guide_name,
                    "keywords": ", ".join(guide_info["keywords"]),
                    "priority": 2  # ê°€ì´ë“œëŠ” ì¤‘ê°„ ìš°ì„ ìˆœìœ„
                }
            ))

        return chunks

    def process_file(self, filename):
        """íŒŒì¼ íƒ€ì…ì— ë”°ë¼ ë‹¤ë¥¸ ì²˜ë¦¬ ë°©ì‹ ì ìš©"""
        if not os.path.exists(filename):
            print(f"   âŒ íŒŒì¼ ì—†ìŒ: {filename}")
            return []

        print(f"\nğŸ“‚ ì²˜ë¦¬ ì¤‘: {filename}")

        try:
            with open(filename, encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            print(f"   âŒ ì½ê¸° ì‹¤íŒ¨: {e}")
            return []

        chunks = []

        if 'detailed' in filename:
            # ìƒì„¸ ìŠ¤í‚¤ë§ˆ: í—¤ë” ê¸°ë°˜ ë¶„í• 
            detail_chunks = self.extract_detailed_chunks(content, filename)
            chunks.extend(detail_chunks)
            print(f"   ğŸ“– ìƒì„¸ ì²­í¬: {len(detail_chunks)}ê°œ")
        else:
            # ê¸°ì¡´ ìŠ¤í‚¤ë§ˆ: ê¸°ì¡´ ë°©ì‹
            table_chunks = self.extract_table_info(content, filename)
            faq_chunks = self.extract_faq_info(content, filename)
            chunks.extend(table_chunks)
            chunks.extend(faq_chunks)
            print(f"   ğŸ“‹ í…Œì´ë¸” ì²­í¬: {len(table_chunks)}ê°œ")
            print(f"   â“ FAQ ì²­í¬: {len(faq_chunks)}ê°œ")

        return chunks

    def process_all_files(self):
        """ëª¨ë“  íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ í•˜ì´ë¸Œë¦¬ë“œ ì²­í¬ ìƒì„±"""
        print("ğŸ”„ í•˜ì´ë¸Œë¦¬ë“œ ì²­í‚¹ ì‹œì‘...")

        total_chunks = 0

        # 1. ê° ìŠ¤í‚¤ë§ˆ íŒŒì¼ ì²˜ë¦¬
        for filename in self.schema_files:
            file_chunks = self.process_file(filename)
            self.all_chunks.extend(file_chunks)
            total_chunks += len(file_chunks)

        # 2. ê´€ê³„ ì •ë³´ ì²­í¬ ìƒì„±
        rel_chunks = self.create_relationship_chunks()
        self.all_chunks.extend(rel_chunks)
        print(f"\nğŸ”— ê´€ê³„ ì²­í¬: {len(rel_chunks)}ê°œ")

        # 3. ë„ë©”ì¸ ê°€ì´ë“œ ì²­í¬ ìƒì„±
        guide_chunks = self.create_domain_guide_chunks()
        self.all_chunks.extend(guide_chunks)
        print(f"ğŸ“– ê°€ì´ë“œ ì²­í¬: {len(guide_chunks)}ê°œ")

        total_chunks += len(rel_chunks) + len(guide_chunks)

        print(f"\nâœ… ì´ {total_chunks}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")
        return self.all_chunks

    def create_vectordb(self, chunks):
        """ë²¡í„° DB ìƒì„±"""
        print("\nğŸ¤– ì„ë² ë”© ëª¨ë¸ ë¡œë”©...")

        # ì„ë² ë”© ëª¨ë¸ ì„¤ì • (ë©€í‹°í„´ í‰ê°€ì— ìµœì í™”)
        embedding = HuggingFaceEmbeddings(
            model_name="intfloat/multilingual-e5-large",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )

        print("ğŸ’¾ ë²¡í„° DB ìƒì„± ì¤‘...")

        # ê¸°ì¡´ DB ì‚­ì œ í›„ ìƒˆë¡œ ìƒì„±
        import shutil
        if os.path.exists("chroma_db"):
            shutil.rmtree("chroma_db")

        # Chroma DB ìƒì„± (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì§€ì›)
        vectordb = Chroma.from_documents(
            chunks,
            embedding,
            persist_directory="./chroma_db",
            collection_metadata={"hnsw:space": "cosine"}
        )

        print("âœ… ë²¡í„° DB ìƒì„± ì™„ë£Œ!")
        return vectordb

    def generate_stats(self, chunks):
        """í†µê³„ ì •ë³´ ìƒì„±"""
        stats = {
            "created_at": datetime.now().isoformat(),
            "total_chunks": len(chunks),
            "chunk_types": {},
            "domains": {},
            "files_processed": list(self.schema_files)
        }

        # íƒ€ì…ë³„/ë„ë©”ì¸ë³„ í†µê³„
        for chunk in chunks:
            chunk_type = chunk.metadata.get('type', 'unknown')
            domain = chunk.metadata.get('domain', 'unknown')

            stats["chunk_types"][chunk_type] = stats["chunk_types"].get(chunk_type, 0) + 1
            stats["domains"][domain] = stats["domains"].get(domain, 0) + 1

        # í†µê³„ íŒŒì¼ ì €ì¥
        with open("hybrid_chunking_stats.json", "w", encoding="utf-8") as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)

        return stats

    def test_hybrid_search(self, vectordb):
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸:")

        test_queries = [
            "í™˜ì ë‚˜ì´ ì •ë³´",  # í™˜ì ë„ë©”ì¸
            "í˜ˆì•• ì¸¡ì • ë°ì´í„°",  # ì´ë²¤íŠ¸ ë„ë©”ì¸
            "í•­ìƒì œ ì²˜ë°©",  # ì•½ë¬¼ ë„ë©”ì¸
            "í…Œì´ë¸” ì—°ê²° ë°©ë²•",  # ê´€ê³„ ì •ë³´
            "ADMISSIONS í…Œì´ë¸” êµ¬ì¡°"  # ìƒì„¸ ì •ë³´
        ]

        for query in test_queries:
            print(f"\nì§ˆë¬¸: '{query}'")
            results = vectordb.similarity_search(query, k=3)

            for i, doc in enumerate(results):
                chunk_type = doc.metadata.get('type', 'unknown')
                domain = doc.metadata.get('domain', 'unknown')
                source = doc.metadata.get('source', 'unknown')

                preview = doc.page_content[:80].replace('\n', ' ')
                print(f"  {i+1}. [{chunk_type}|{domain}] {preview}...")


def create_hybrid_embeddings():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ ì²­í‚¹ ì„ë² ë”© ìƒì„± ì‹œì‘")

    # ì²­í‚¹ ê°ì²´ ìƒì„±
    chunker = HybridSchemaChunker()

    # ëª¨ë“  ì²­í¬ ìƒì„±
    all_chunks = chunker.process_all_files()

    if not all_chunks:
        print("âŒ ìƒì„±ëœ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

    # ë²¡í„° DB ìƒì„±
    vectordb = chunker.create_vectordb(all_chunks)

    # í†µê³„ ìƒì„±
    stats = chunker.generate_stats(all_chunks)
    print(f"\nğŸ“Š í†µê³„ ì •ë³´:")
    print(f"   â”” ì´ ì²­í¬: {stats['total_chunks']}ê°œ")
    for chunk_type, count in stats['chunk_types'].items():
        print(f"   â”” {chunk_type}: {count}ê°œ")

    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    chunker.test_hybrid_search(vectordb)

    print("\nğŸ‰ í•˜ì´ë¸Œë¦¬ë“œ ì²­í‚¹ ì™„ë£Œ!")
    return vectordb


if __name__ == "__main__":
    # í•˜ì´ë¸Œë¦¬ë“œ ì„ë² ë”© ìƒì„± ì‹¤í–‰
    vectordb = create_hybrid_embeddings()